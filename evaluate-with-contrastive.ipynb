{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-pm9GzOk9tMf",
    "outputId": "8431e4e6-8ec3-41be-9c64-d582f8dade4d"
   },
   "outputs": [],
   "source": [
    "# Set the path to your data directory on Google Drive\n",
    "data_dir = './voc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thtfI8d0ad4q"
   },
   "source": [
    "# Download and unzip the PascalVoc dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "ivkItYJooBYF",
    "outputId": "c787603f-3fcc-4c1b-ae42-c9dad86c22df"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "train_dataset = torchvision.datasets.VOCSegmentation(root=data_dir, year='2007', download=False, image_set='train')\n",
    "val_dataset = torchvision.datasets.VOCSegmentation(root=data_dir,year='2007',download=False, image_set='val')\n",
    "\n",
    "# test_dataset = torchvision.datasets.VOCSegmentation(root='./data',year='2007',download=True,image_set='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uKjin4TeaiYN"
   },
   "source": [
    "# Data Preparation\n",
    "\n",
    "run the code below to get thre dataloader objects, namely: train_loader, val_loader and test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q1UV1V2XqkWf",
    "outputId": "04a90050-9610-4011-d28a-6adda25524e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils import data\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "\n",
    "num_classes = 21\n",
    "ignore_label = 255\n",
    "root = data_dir\n",
    "\n",
    "'''\n",
    "color map\n",
    "0=background, 1=aeroplane, 2=bicycle, 3=bird, 4=boat, 5=bottle # 6=bus, 7=car, 8=cat, 9=chair, 10=cow, 11=diningtable,\n",
    "12=dog, 13=horse, 14=motorbike, 15=person # 16=potted plant, 17=sheep, 18=sofa, 19=train, 20=tv/monitor\n",
    "'''\n",
    "\n",
    "\n",
    "#Feel free to convert this palette to a map\n",
    "palette = [0, 0, 0, 128, 0, 0, 0, 128, 0, 128, 128, 0, 0, 0, 128, 128, 0, 128, 0, 128, 128,\n",
    "           128, 128, 128, 64, 0, 0, 192, 0, 0, 64, 128, 0, 192, 128, 0, 64, 0, 128, 192, 0, 128,\n",
    "           64, 128, 128, 192, 128, 128, 0, 64, 0, 128, 64, 0, 0, 192, 0, 128, 192, 0, 0, 64, 128]  #3 values- R,G,B for every class. First 3 values for class 0, next 3 for\n",
    "#class 1 and so on......\n",
    "\n",
    "'''\n",
    "Depending on the mode, train or val or test, the function reads the train.txt, val.txt and test.txt files and returns a list of tuples of the form\n",
    "(image_path, mask_path) for each image in the dataset, where image_path is the path to the image and mask_path is the path to the mask for that image. \n",
    "'''\n",
    "def make_dataset(mode):\n",
    "    assert mode in ['train', 'val', 'test']\n",
    "    items = []\n",
    "    if mode == 'train':\n",
    "        img_path = os.path.join(root, 'VOCdevkit', 'VOC2007', 'JPEGImages')\n",
    "        mask_path = os.path.join(root, 'VOCdevkit', 'VOC2007', 'SegmentationClass')\n",
    "        data_list = [l.strip('\\n') for l in open(os.path.join(\n",
    "            root, 'VOCdevkit', 'VOC2007', 'ImageSets', 'Segmentation', 'train.txt')).readlines()]\n",
    "        for it in data_list:\n",
    "            item = (os.path.join(img_path, it + '.jpg'), os.path.join(mask_path, it + '.png'))\n",
    "            items.append(item)\n",
    "    elif mode == 'val':\n",
    "        img_path = os.path.join(root, 'VOCdevkit', 'VOC2007', 'JPEGImages')\n",
    "        mask_path = os.path.join(root, 'VOCdevkit', 'VOC2007', 'SegmentationClass')\n",
    "        data_list = [l.strip('\\n') for l in open(os.path.join(\n",
    "            root, 'VOCdevkit', 'VOC2007', 'ImageSets', 'Segmentation', 'val.txt')).readlines()]\n",
    "        for it in data_list:\n",
    "            item = (os.path.join(img_path, it + '.jpg'), os.path.join(mask_path, it + '.png'))\n",
    "            items.append(item)\n",
    "    else:\n",
    "        img_path = os.path.join(root, 'VOCdevkit', 'VOC2007', 'JPEGImages')\n",
    "        mask_path = os.path.join(root, 'VOCdevkit', 'VOC2007', 'SegmentationClass')\n",
    "        data_list = [l.strip('\\n') for l in open(os.path.join(\n",
    "            root, 'VOCdevkit', 'VOC2007', 'ImageSets', 'Segmentation', 'test.txt')).readlines()]\n",
    "        for it in data_list:\n",
    "            item = (os.path.join(img_path, it + '.jpg'), os.path.join(mask_path, it + '.png'))\n",
    "            items.append(item)\n",
    "    return items\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "The class VOC is a subclass of the class torch.utils.data.Dataset. It overrides the __len__ and __getitem__ methods.\n",
    "The __len__ method returns the length of the dataset, i.e. the number of images in the dataset.\n",
    "The __getitem__ method returns the image and the mask for the given index.\n",
    "'''\n",
    "\n",
    "class VOC(data.Dataset):\n",
    "    def __init__(self, mode, transform=None, target_transform=None, common_transform=None):\n",
    "        self.imgs = make_dataset(mode)        \n",
    "        if len(self.imgs) == 0:\n",
    "            raise RuntimeError('Found 0 images, please check the data set')\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.common_transform = common_transform\n",
    "        self.width = 256\n",
    "        self.height = 256\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        img_path, mask_path = self.imgs[index]\n",
    "        img = Image.open(img_path).convert('RGB').resize((self.width, self.height))\n",
    "        mask = Image.open(mask_path).resize((self.width, self.height))\n",
    "\n",
    "        if self.common_transform is not None:\n",
    "            img, mask = self.common_transform((img,mask)) \n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            mask = self.target_transform(mask)\n",
    "\n",
    "        mask[mask==ignore_label]=0\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    voc = VOC(\"train\")\n",
    "\n",
    "    print(len(voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "F9PiYKnVqZyn"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "class MaskToTensor(object):\n",
    "    def __call__(self, img):\n",
    "        return torch.from_numpy(np.array(img, dtype=np.int32)).long()\n",
    "\n",
    "\n",
    "mean_std = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "input_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(*mean_std)\n",
    "])\n",
    "\n",
    "target_transform = transforms.Compose([\n",
    "    MaskToTensor()\n",
    "])\n",
    "\n",
    "original_train_dataset = VOC('train', transform=input_transform, target_transform=target_transform)\n",
    "original_val_dataset = VOC('val', transform=input_transform, target_transform=target_transform)\n",
    "# original_test_dataset = VOC('test', transform=input_transform, target_transform=target_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sRm6BoBqxdzw",
    "outputId": "f68df7c5-b344-4b7e-a047-5f3cc46f4756"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "NUM_WORKERS = 2\n",
    "PREFETCH_FACTOR = 2 # improves data transfer speed between GPU and CPU and reduces GPU wait time\n",
    "train_loader = DataLoader(dataset=original_train_dataset, batch_size=16, shuffle=True, num_workers=NUM_WORKERS, prefetch_factor=PREFETCH_FACTOR, pin_memory=True)\n",
    "val_loader = DataLoader(dataset=original_val_dataset, batch_size=16, shuffle=False, num_workers=NUM_WORKERS, prefetch_factor=PREFETCH_FACTOR, pin_memory=True)\n",
    "\n",
    "# test_loader = DataLoader(dataset=original_test_dataset, batch_size= 16, shuffle=False, num_workers=NUM_WORKERS, prefetch_factor=PREFETCH_FACTOR, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCiDan5JzXVS"
   },
   "source": [
    "# utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Y4wq8Kp_UtHf"
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "def iou(pred, target, n_classes = 21):\n",
    "    target[target==255] = 0\n",
    "\n",
    "    ious = []\n",
    "\n",
    "    for cls in range(n_classes):\n",
    "        intersection = torch.sum((pred == cls) & (target == cls)).item()\n",
    "        union = torch.sum(pred == cls) + torch.sum(target == cls) - intersection\n",
    "        union = union.item()\n",
    "        if union!=0:\n",
    "            ious.append(intersection/union)\n",
    "\n",
    "    ious = np.array(ious)\n",
    "    return ious\n",
    "\n",
    "'''\n",
    "returns pixel accuracy for the batch\n",
    "'''\n",
    "def pixel_acc(pred, target):\n",
    "    target[target==255] = 0\n",
    "    \n",
    "    correct = torch.sum(pred==target).item()\n",
    "    total_predictions = target.shape[0]*target.shape[1]*target.shape[2]\n",
    "    return correct/total_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nIMKgG9GRiel"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.nn.functional as F\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model=None):\n",
    "\n",
    "    model_ = model \n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    \n",
    "    best_iou_score = 0.0\n",
    "\n",
    "    trainEpochLoss = []\n",
    "    trainEpochAccuracy = []\n",
    "    trainEpochIOU = []\n",
    "    valEpochLoss = []\n",
    "    valEpochAccuracy = []\n",
    "    valEpochIOU = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "        train_loss = []\n",
    "        train_acc = []\n",
    "        train_iou = []\n",
    "\n",
    "        ts = time.time()\n",
    "        for itr, (inputs, labels) in enumerate(train_loader):\n",
    "            #   reset optimizer gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs =  inputs.to(device)\n",
    "            labels =   labels.to(device)\n",
    "\n",
    "            trainOutputs =  model_(inputs)\n",
    "#             trainOutputs = F.softmax(trainOutputs, dim=1)\n",
    "            loss =  criterion(trainOutputs,labels)  #  calculate loss\n",
    "            loss.backward()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # To compute accuracy and IOU\n",
    "                # outputs = F.log_softmax(model_(inputs), dim=1)\n",
    "                _, pred = torch.max(trainOutputs, dim=1)\n",
    "                \n",
    "                train_iou += [np.mean(iou(pred, labels))]\n",
    "                train_acc += [pixel_acc(pred, labels)]\n",
    "                train_loss.append(loss.item())\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if itr % 10 == 0:\n",
    "                print(f\"==> epoch{epoch}, iter{itr+1}, Train set=> loss: {train_loss[-1]}, IOU: {train_iou[-1]}, Acc: {train_acc[-1]}\")\n",
    "\n",
    "        # print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n",
    "\n",
    "        print(f\"Finish epoch {epoch}, time elapsed {time.time() - ts}\")\n",
    "\n",
    "        val_loss, val_iou, val_acc = val(epoch,model_)\n",
    "        model_.train()\n",
    "        \n",
    "        \n",
    "        ##### Plotting values\n",
    "        trainEpochLoss.append(np.mean(np.asarray(train_loss)))\n",
    "        trainEpochIOU.append(np.mean(np.asarray(train_iou)))\n",
    "        trainEpochAccuracy.append(np.mean(train_acc))\n",
    "        valEpochLoss.append(val_loss)\n",
    "        valEpochIOU.append(val_iou)\n",
    "        valEpochAccuracy.append(val_acc)\n",
    "\n",
    "    # plots(trainEpochLoss, trainEpochAccuracy, trainEpochIOU, valEpochLoss, valEpochAccuracy, valEpochIOU, best_iter, saveLocation=saveLocation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "UTQQ4B7hU3cd"
   },
   "outputs": [],
   "source": [
    "def val(epoch, model=None):\n",
    "    model_ = model\n",
    "    model_.eval() # Put in eval mode (disables batchnorm/dropout) !\n",
    "    \n",
    "    losses = []\n",
    "    mean_iou_scores = []\n",
    "    accuracy = []\n",
    "\n",
    "    with torch.no_grad(): # we don't need to calculate the gradient in the validation/testing\n",
    "        num_iter = 0\n",
    "        for iter, (inputs, labels) in enumerate(val_loader):\n",
    "            \n",
    "            # both inputs and labels have to reside in the same device as the model's\n",
    "            inputs =  inputs.to(device)#  transfer the input to the same device as the model's\n",
    "            labels =   labels.to(device)#  transfer the labels to the same device as the model's\n",
    "\n",
    "\n",
    "            outputs = model_(inputs)\n",
    "            outputs = F.softmax(outputs, dim=1)\n",
    "#             valoutputs = model_(inputs)\n",
    "            valloss = criterion(outputs, labels)\n",
    "            \n",
    "            num_iter += 1\n",
    "            _, pred = torch.max(outputs, dim=1)\n",
    "            mean_iou_scores += [np.mean(iou(pred, labels))]\n",
    "            accuracy += [pixel_acc(pred, labels)]\n",
    "            losses += [valloss.item()]\n",
    "\n",
    "    # print(mean_iou_scores, accuracy)\n",
    "    print(f\"=========> Loss at epoch {epoch} is {np.mean(losses)}\")\n",
    "    print(f\"=========> IoU at epoch {epoch} is {np.mean(mean_iou_scores)}\")\n",
    "    print(f\"=========> Pixel acc at epoch {epoch} is {np.mean(accuracy)}\")\n",
    "\n",
    "    \n",
    "    return np.mean(losses), np.mean(mean_iou_scores), np.mean(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "loHUcdGAb_Xy"
   },
   "source": [
    "# SSL models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xkWyigy-PQhI"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(self, in_chan, out_dim):\n",
    "        super(Backbone, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=in_chan, out_channels=16, kernel_size=3, stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu1 = nn.ReLU()\n",
    "#         self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.relu2 = nn.ReLU()\n",
    "#         self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "#         self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.maxpool5 = nn.MaxPool2d(kernel_size=2, stride=2)        \n",
    "        \n",
    "        self.pooling = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(in_features=256, out_features=out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.bn1(self.conv1(x)))\n",
    "        x = self.relu2(self.bn2(self.conv2(x)))\n",
    "        x = self.relu3(self.bn3(self.conv3(x)))\n",
    "\n",
    "        x = self.relu4(self.bn4(self.conv4(x)))\n",
    "        x = self.maxpool4(x)\n",
    "        \n",
    "        x = self.relu5(self.bn5(self.conv5(x)))\n",
    "        x = self.maxpool5(x)\n",
    "\n",
    "        x = self.pooling(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "_1Zwqss-PIJl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class UNet_ssl(nn.Module):\n",
    "    def __init__(self, n_class=21, n_dim=512):\n",
    "        super(UNet_ssl, self).__init__()\n",
    "\n",
    "        # Encoder (Based on the provided SSL architecture)\n",
    "        self.encoder = Backbone(in_chan=3, out_dim=n_dim)\n",
    "        \n",
    "#         self.reducer = nn.Conv2d(256, 8, kernel_size=1)\n",
    "        self.drastic = nn.ConvTranspose2d(256, 256, kernel_size=(16,16), stride = 1)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder4 = self.expanding_block(256, 128)\n",
    "        self.decoder3 = self.expanding_block(128, 64)\n",
    "        self.decoder2 = self.expanding_block(64, 32)\n",
    "        self.decoder1 = self.expanding_block(32, 16)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output = nn.Conv2d(16, n_class, kernel_size=1)\n",
    "        \n",
    "    def expanding_block(self, in_channels, out_channels):\n",
    "        block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(out_channels, out_channels, kernel_size=2, stride=2)\n",
    "        )\n",
    "        return block\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder (SSL model)\n",
    "        features = self.encoder(x)\n",
    "        \n",
    "#         features = self.reducer(features)\n",
    "        features = self.drastic(features)\n",
    "        \n",
    "        # Decoder\n",
    "        decode4 = self.decoder4(features)\n",
    "        decode3 = self.decoder3(decode4)\n",
    "        decode2 = self.decoder2(decode3)\n",
    "        decode1 = self.decoder1(decode2)\n",
    "        \n",
    "        # Output\n",
    "        output = self.output(decode1)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QZABMvHyKK0Y",
    "outputId": "0cb8e56d-9b26-45c5-dffb-4d21c4422392"
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming 21 classes for segmentation\n",
    "num_classes = 21\n",
    "\n",
    "# Create the U-Net model\n",
    "unet_ssl_model = UNet_ssl(n_class=num_classes, n_dim=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SSL model weights\n",
    "ssl_model_weights = torch.load('pet_data_e250.pth', map_location=torch.device(device))\n",
    "# unet_ssl_model.encoder.load_state_dict(ssl_model_weights)\n",
    "\n",
    "# freeze the encoder\n",
    "# for param in unet_ssl_model.encoder.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "optimizer = optim.Adam(unet_ssl_model.parameters(), lr=0.005)\n",
    "unet_ssl_model = unet_ssl_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "id": "GbCVDRxGPIRF",
    "outputId": "bcff3e81-c437-45f8-a6fa-5ca8a16ab5b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> epoch0, iter1, Train set=> loss: 2.9869484901428223, IOU: 0.0006705064039963942, Acc: 0.008716583251953125\n",
      "==> epoch0, iter11, Train set=> loss: 2.8021340370178223, IOU: 0.04569166898727417, Acc: 0.7310667037963867\n",
      "Finish epoch 0, time elapsed 3.6258704662323\n",
      "=========> Loss at epoch 0 is 2.9568283898489818\n",
      "=========> IoU at epoch 0 is 0.031231075749129058\n",
      "=========> Pixel acc at epoch 0 is 0.36916161945887976\n",
      "==> epoch1, iter1, Train set=> loss: 2.470890522003174, IOU: 0.03513728335283783, Acc: 0.5706253051757812\n",
      "==> epoch1, iter11, Train set=> loss: 2.1337180137634277, IOU: 0.05330058506556919, Acc: 0.7462081909179688\n",
      "Finish epoch 1, time elapsed 3.5921332836151123\n",
      "=========> Loss at epoch 1 is 2.3768729312079295\n",
      "=========> IoU at epoch 1 is 0.05565189862198441\n",
      "=========> Pixel acc at epoch 1 is 0.7508762087140765\n",
      "==> epoch2, iter1, Train set=> loss: 4.3581461906433105, IOU: 0.05216400439922626, Acc: 0.6781320571899414\n",
      "==> epoch2, iter11, Train set=> loss: 1.5620344877243042, IOU: 0.05195265549879808, Acc: 0.675384521484375\n",
      "Finish epoch 2, time elapsed 3.5888524055480957\n",
      "=========> Loss at epoch 2 is 2.653474041393825\n",
      "=========> IoU at epoch 2 is 0.05565189862198441\n",
      "=========> Pixel acc at epoch 2 is 0.7508762087140765\n",
      "==> epoch3, iter1, Train set=> loss: 1.3416459560394287, IOU: 0.04733871011173024, Acc: 0.8047580718994141\n",
      "==> epoch3, iter11, Train set=> loss: 1.345975637435913, IOU: 0.05308208465576172, Acc: 0.7962312698364258\n",
      "Finish epoch 3, time elapsed 3.5681474208831787\n",
      "=========> Loss at epoch 3 is 2.463371651513236\n",
      "=========> IoU at epoch 3 is 0.05565189862198441\n",
      "=========> Pixel acc at epoch 3 is 0.7508762087140765\n",
      "==> epoch4, iter1, Train set=> loss: 1.6194684505462646, IOU: 0.05877377436711238, Acc: 0.7640590667724609\n",
      "==> epoch4, iter11, Train set=> loss: 1.2115426063537598, IOU: 0.049830496311187744, Acc: 0.7972879409790039\n",
      "Finish epoch 4, time elapsed 3.6064138412475586\n",
      "=========> Loss at epoch 4 is 2.6920476300375804\n",
      "=========> IoU at epoch 4 is 0.05565189862198441\n",
      "=========> Pixel acc at epoch 4 is 0.7508762087140765\n",
      "==> epoch5, iter1, Train set=> loss: 1.275527834892273, IOU: 0.04762524366378784, Acc: 0.7620038986206055\n",
      "==> epoch5, iter11, Train set=> loss: 1.1349072456359863, IOU: 0.0508694052696228, Acc: 0.8139104843139648\n",
      "Finish epoch 5, time elapsed 3.585643768310547\n",
      "=========> Loss at epoch 5 is 2.5144707645688738\n",
      "=========> IoU at epoch 5 is 0.05565189862198441\n",
      "=========> Pixel acc at epoch 5 is 0.7508762087140765\n",
      "==> epoch6, iter1, Train set=> loss: 0.9040176272392273, IOU: 0.055931345621744795, Acc: 0.8389701843261719\n",
      "==> epoch6, iter11, Train set=> loss: 1.33820641040802, IOU: 0.04652849833170573, Acc: 0.6979274749755859\n",
      "Finish epoch 6, time elapsed 3.584833860397339\n",
      "=========> Loss at epoch 6 is 2.3884958028793335\n",
      "=========> IoU at epoch 6 is 0.05565189862198441\n",
      "=========> Pixel acc at epoch 6 is 0.7508762087140765\n",
      "==> epoch7, iter1, Train set=> loss: 3.3080532550811768, IOU: 0.038304027758146585, Acc: 0.7277765274047852\n",
      "==> epoch7, iter11, Train set=> loss: 1.750443458557129, IOU: 0.05147804532732282, Acc: 0.7206926345825195\n",
      "Finish epoch 7, time elapsed 3.5824429988861084\n",
      "=========> Loss at epoch 7 is 2.707779884338379\n",
      "=========> IoU at epoch 7 is 0.05565189862198441\n",
      "=========> Pixel acc at epoch 7 is 0.7508762087140765\n",
      "==> epoch8, iter1, Train set=> loss: 1.2728850841522217, IOU: 0.05220737457275391, Acc: 0.7831106185913086\n",
      "==> epoch8, iter11, Train set=> loss: 1.3655344247817993, IOU: 0.04376441240310669, Acc: 0.700230598449707\n",
      "Finish epoch 8, time elapsed 3.580972194671631\n",
      "=========> Loss at epoch 8 is 2.503169366291591\n",
      "=========> IoU at epoch 8 is 0.05565189862198441\n",
      "=========> Pixel acc at epoch 8 is 0.7508762087140765\n",
      "==> epoch9, iter1, Train set=> loss: 1.5362842082977295, IOU: 0.03761964374118381, Acc: 0.6771535873413086\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "train(unet_ssl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
