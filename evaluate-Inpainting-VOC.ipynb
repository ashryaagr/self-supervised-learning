{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thtfI8d0ad4q"
   },
   "source": [
    "# Download and unzip the PascalVoc dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "ivkItYJooBYF",
    "outputId": "c787603f-3fcc-4c1b-ae42-c9dad86c22df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/VOCtrainval_06-Nov-2007.tar\n",
      "Extracting ./data/VOCtrainval_06-Nov-2007.tar to ./data\n",
      "Using downloaded and verified file: ./data/VOCtrainval_06-Nov-2007.tar\n",
      "Extracting ./data/VOCtrainval_06-Nov-2007.tar to ./data\n",
      "Using downloaded and verified file: ./data/VOCtest_06-Nov-2007.tar\n",
      "Extracting ./data/VOCtest_06-Nov-2007.tar to ./data\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "train_dataset =torchvision.datasets.VOCSegmentation(root='./data',year='2007',download=True,image_set='train')\n",
    "val_dataset = torchvision.datasets.VOCSegmentation(root='./data',year='2007',download=True,image_set='val')\n",
    "test_dataset = torchvision.datasets.VOCSegmentation(root='./data',year='2007',download=True,image_set='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "W7O2K34Opfen"
   },
   "outputs": [],
   "source": [
    "!cd data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sBPY-IXXn1g4",
    "outputId": "f0c2ed64-22b3-40f7-97a1-e55fa6a2aa13"
   },
   "outputs": [],
   "source": [
    "# !wget http://pjreddie.com/media/files/VOCtest_06-Nov-2007.tar\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "y-ooO_RNn1jb"
   },
   "outputs": [],
   "source": [
    "# !tar -xvf VOCtest_06-Nov-2007.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uKjin4TeaiYN"
   },
   "source": [
    "# Data Preparation\n",
    "\n",
    "run the code below to get thre dataloader objects, namely: train_loader, val_loader and test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q1UV1V2XqkWf",
    "outputId": "04a90050-9610-4011-d28a-6adda25524e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils import data\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "\n",
    "num_classes = 21\n",
    "ignore_label = 255\n",
    "root = './data'\n",
    "\n",
    "'''\n",
    "color map\n",
    "0=background, 1=aeroplane, 2=bicycle, 3=bird, 4=boat, 5=bottle # 6=bus, 7=car, 8=cat, 9=chair, 10=cow, 11=diningtable,\n",
    "12=dog, 13=horse, 14=motorbike, 15=person # 16=potted plant, 17=sheep, 18=sofa, 19=train, 20=tv/monitor\n",
    "'''\n",
    "\n",
    "\n",
    "#Feel free to convert this palette to a map\n",
    "palette = [0, 0, 0, 128, 0, 0, 0, 128, 0, 128, 128, 0, 0, 0, 128, 128, 0, 128, 0, 128, 128,\n",
    "           128, 128, 128, 64, 0, 0, 192, 0, 0, 64, 128, 0, 192, 128, 0, 64, 0, 128, 192, 0, 128,\n",
    "           64, 128, 128, 192, 128, 128, 0, 64, 0, 128, 64, 0, 0, 192, 0, 128, 192, 0, 0, 64, 128]  #3 values- R,G,B for every class. First 3 values for class 0, next 3 for\n",
    "#class 1 and so on......\n",
    "\n",
    "'''\n",
    "Depending on the mode, train or val or test, the function reads the train.txt, val.txt and test.txt files and returns a list of tuples of the form\n",
    "(image_path, mask_path) for each image in the dataset, where image_path is the path to the image and mask_path is the path to the mask for that image. \n",
    "'''\n",
    "def make_dataset(mode):\n",
    "    assert mode in ['train', 'val', 'test']\n",
    "    items = []\n",
    "    if mode == 'train':\n",
    "        img_path = os.path.join(root, 'VOCdevkit', 'VOC2007', 'JPEGImages')\n",
    "        mask_path = os.path.join(root, 'VOCdevkit', 'VOC2007', 'SegmentationClass')\n",
    "        data_list = [l.strip('\\n') for l in open(os.path.join(\n",
    "            root, 'VOCdevkit', 'VOC2007', 'ImageSets', 'Segmentation', 'train.txt')).readlines()]\n",
    "        for it in data_list:\n",
    "            item = (os.path.join(img_path, it + '.jpg'), os.path.join(mask_path, it + '.png'))\n",
    "            items.append(item)\n",
    "    elif mode == 'val':\n",
    "        img_path = os.path.join(root, 'VOCdevkit', 'VOC2007', 'JPEGImages')\n",
    "        mask_path = os.path.join(root, 'VOCdevkit', 'VOC2007', 'SegmentationClass')\n",
    "        data_list = [l.strip('\\n') for l in open(os.path.join(\n",
    "            root, 'VOCdevkit', 'VOC2007', 'ImageSets', 'Segmentation', 'val.txt')).readlines()]\n",
    "        for it in data_list:\n",
    "            item = (os.path.join(img_path, it + '.jpg'), os.path.join(mask_path, it + '.png'))\n",
    "            items.append(item)\n",
    "    else:\n",
    "        img_path = os.path.join(root, 'VOCdevkit', 'VOC2007', 'JPEGImages')\n",
    "        mask_path = os.path.join(root, 'VOCdevkit', 'VOC2007', 'SegmentationClass')\n",
    "        data_list = [l.strip('\\n') for l in open(os.path.join(\n",
    "            root, 'VOCdevkit', 'VOC2007', 'ImageSets', 'Segmentation', 'test.txt')).readlines()]\n",
    "        for it in data_list:\n",
    "            item = (os.path.join(img_path, it + '.jpg'), os.path.join(mask_path, it + '.png'))\n",
    "            items.append(item)\n",
    "    return items\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "The class VOC is a subclass of the class torch.utils.data.Dataset. It overrides the __len__ and __getitem__ methods.\n",
    "The __len__ method returns the length of the dataset, i.e. the number of images in the dataset.\n",
    "The __getitem__ method returns the image and the mask for the given index.\n",
    "'''\n",
    "\n",
    "class VOC(data.Dataset):\n",
    "    def __init__(self, mode, transform=None, target_transform=None, common_transform=None):\n",
    "        self.imgs = make_dataset(mode)\n",
    "        if len(self.imgs) == 0:\n",
    "            raise RuntimeError('Found 0 images, please check the data set')\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.common_transform = common_transform\n",
    "        self.width = 224\n",
    "        self.height = 224\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        img_path, mask_path = self.imgs[index]\n",
    "        img = Image.open(img_path).convert('RGB').resize((self.width, self.height))\n",
    "        mask = Image.open(mask_path).resize((self.width, self.height))\n",
    "\n",
    "        if self.common_transform is not None:\n",
    "            img, mask = self.common_transform((img,mask)) \n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            mask = self.target_transform(mask)\n",
    "\n",
    "        mask[mask==ignore_label]=0\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    voc = VOC(\"train\")\n",
    "\n",
    "    print(len(voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "F9PiYKnVqZyn"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "class MaskToTensor(object):\n",
    "    def __call__(self, img):\n",
    "        return torch.from_numpy(np.array(img, dtype=np.int32)).long()\n",
    "\n",
    "\n",
    "mean_std = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "# common_transform = transforms.Compose([\n",
    "#     # voc.MirrorFlip(0.5),\n",
    "#     # voc.Rotate(10),\n",
    "#     # voc.CenterCrop(180)\n",
    "# ])\n",
    "\n",
    "input_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(*mean_std)\n",
    "])\n",
    "\n",
    "target_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    MaskToTensor()\n",
    "])\n",
    "\n",
    "# augmented_train_dataset =voc.VOC('train', transform=input_transform, target_transform=target_transform, common_transform=common_transform)\n",
    "# augmented_val_dataset = voc.VOC('val', transform=input_transform, target_transform=target_transform, common_transform=common_transform)\n",
    "# augmented_test_dataset = voc.VOC('test', transform=input_transform, target_transform=target_transform, common_transform=common_transform)\n",
    "\n",
    "original_train_dataset =VOC('train', transform=input_transform, target_transform=target_transform)\n",
    "original_val_dataset = VOC('val', transform=input_transform, target_transform=target_transform)\n",
    "original_test_dataset = VOC('test', transform=input_transform, target_transform=target_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sRm6BoBqxdzw",
    "outputId": "f68df7c5-b344-4b7e-a047-5f3cc46f4756"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "NUM_WORKERS = 4\n",
    "PREFETCH_FACTOR = 2 # improves data transfer speed between GPU and CPU and reduces GPU wait time\n",
    "train_loader = DataLoader(dataset=original_train_dataset, batch_size= 16, shuffle=True, num_workers=NUM_WORKERS, prefetch_factor=PREFETCH_FACTOR, pin_memory=True)\n",
    "val_loader = DataLoader(dataset=original_val_dataset, batch_size= 16, shuffle=False, num_workers=NUM_WORKERS, prefetch_factor=PREFETCH_FACTOR, pin_memory=True)\n",
    "test_loader = DataLoader(dataset=original_test_dataset, batch_size= 16, shuffle=False, num_workers=NUM_WORKERS, prefetch_factor=PREFETCH_FACTOR, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5AiZtwNTbLBE"
   },
   "source": [
    "# end of data section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "IXJsPn8RynHU",
    "outputId": "1e28c02e-26a5-4d25-a0fa-5a23803b541a"
   },
   "outputs": [],
   "source": [
    "# for images, masks in train_loader:\n",
    "#   image = images.to(device)\n",
    "#   mask = masks.to(device)\n",
    "#   break\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# # Assuming data is a tuple (image, label)\n",
    "# image = images[0]\n",
    "# # image = masks[0]\n",
    "# # Convert the tensor image to NumPy array and transpose the dimensions\n",
    "# image = image.numpy().transpose((1, 2, 0))\n",
    "\n",
    "# # Denormalize the image (if it was normalized during transformation)\n",
    "# mean = (0.5, 0.5, 0.5)  # Mean used for normalization\n",
    "# std = (0.5, 0.5, 0.5)  # Standard deviation used for normalization\n",
    "# image = image * std + mean\n",
    "\n",
    "# # Clip the pixel values to [0, 1] range in case of any numerical instability\n",
    "# image = np.clip(image, 0, 1)\n",
    "\n",
    "# # Plot the image\n",
    "# plt.imshow(image)\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCiDan5JzXVS"
   },
   "source": [
    "# utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Y4wq8Kp_UtHf"
   },
   "outputs": [],
   "source": [
    "def iou(pred, target, n_classes = 21):\n",
    "    target[target==255] = 0\n",
    "\n",
    "    ious = []\n",
    "\n",
    "    for cls in range(n_classes):\n",
    "        intersection = torch.sum((pred == cls) & (target == cls)).item()\n",
    "        union = torch.sum(pred == cls) + torch.sum(target == cls) - intersection\n",
    "        union = union.item()\n",
    "        if union!=0:\n",
    "            ious.append(intersection/union)\n",
    "\n",
    "    ious = np.array(ious)\n",
    "    return ious\n",
    "\n",
    "'''\n",
    "returns pixel accuracy for the batch\n",
    "'''\n",
    "def pixel_acc(pred, target):\n",
    "    target[target==255] = 0\n",
    "    \n",
    "    correct = torch.sum(pred==target).item()\n",
    "    total_predictions = target.shape[0]*target.shape[1]*target.shape[2]\n",
    "    return correct/total_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "nIMKgG9GRiel"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.nn.functional as F\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model=None):\n",
    "\n",
    "    model_ = model \n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    \n",
    "    best_iou_score = 0.0\n",
    "\n",
    "    trainEpochLoss = []\n",
    "    trainEpochAccuracy = []\n",
    "    trainEpochIOU = []\n",
    "    valEpochLoss = []\n",
    "    valEpochAccuracy = []\n",
    "    valEpochIOU = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "        train_loss = []\n",
    "        train_acc = []\n",
    "        train_iou = []\n",
    "\n",
    "        ts = time.time()\n",
    "        for iter, (inputs, labels) in enumerate(train_loader):\n",
    "            #   reset optimizer gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            # both inputs and labels have to reside in the same device as the model's\n",
    "            inputs =  inputs.to(device)#  transfer the input to the same device as the model's\n",
    "            labels =   labels.to(device)#  transfer the labels to the same device as the model's\n",
    "\n",
    "            trainOutputs =  model_.forward(inputs) #   Compute outputs. we will not need to transfer the output, it will be automatically in the same device as the model's!\n",
    "            trainOutputs = F.softmax(trainOutputs)\n",
    "            loss =  criterion(trainOutputs,labels)  #  calculate loss\n",
    "            loss.backward()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # To compute accuracy and IOU\n",
    "                # outputs = F.log_softmax(model_(inputs), dim=1)\n",
    "                _, pred = torch.max(trainOutputs, dim=1)\n",
    "                \n",
    "                train_iou += [np.mean(iou(pred, labels))]\n",
    "                train_acc += [pixel_acc(pred, labels)]\n",
    "                train_loss.append(loss.item())\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if iter % 10 == 0:\n",
    "                print(f\"==> epoch{epoch}, iter{iter}, Train set=> loss: {train_loss[-1]}, IOU: {train_iou[-1]}, Acc: {train_acc[-1]}\")\n",
    "\n",
    "        # print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n",
    "\n",
    "        print(\"Finish epoch {}, time elapsed {}\".format(epoch, time.time() - ts))\n",
    "\n",
    "        val_loss, val_iou, val_acc = val(epoch,model_)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ##### Plotting values\n",
    "        trainEpochLoss.append(np.mean(np.asarray(train_loss)))\n",
    "        trainEpochIOU.append(np.mean(np.asarray(train_iou)))\n",
    "        trainEpochAccuracy.append(np.mean(train_acc))\n",
    "        valEpochLoss.append(val_loss)\n",
    "        valEpochIOU.append(val_iou)\n",
    "        valEpochAccuracy.append(val_acc)\n",
    "\n",
    "    # plots(trainEpochLoss, trainEpochAccuracy, trainEpochIOU, valEpochLoss, valEpochAccuracy, valEpochIOU, best_iter, saveLocation=saveLocation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "UTQQ4B7hU3cd"
   },
   "outputs": [],
   "source": [
    "def val(epoch, model=None):\n",
    "    model_ = model\n",
    "    model_.eval() # Put in eval mode (disables batchnorm/dropout) !\n",
    "    \n",
    "    losses = []\n",
    "    mean_iou_scores = []\n",
    "    accuracy = []\n",
    "\n",
    "    with torch.no_grad(): # we don't need to calculate the gradient in the validation/testing\n",
    "        num_iter = 0\n",
    "        for iter, (inputs, labels) in enumerate(val_loader):\n",
    "            \n",
    "            # both inputs and labels have to reside in the same device as the model's\n",
    "            inputs =  inputs.to(device)#  transfer the input to the same device as the model's\n",
    "            labels =   labels.to(device)#  transfer the labels to the same device as the model's\n",
    "\n",
    "\n",
    "            outputs = F.softmax(model_(inputs), dim=1)\n",
    "#             valoutputs = model_(inputs)\n",
    "            valloss = criterion(outputs, labels)\n",
    "            \n",
    "            num_iter += 1\n",
    "            _, pred = torch.max(outputs, dim=1)\n",
    "            mean_iou_scores += [np.mean(iou(pred, labels))]\n",
    "            accuracy += [pixel_acc(pred, labels)]\n",
    "            losses += [valloss.item()]\n",
    "\n",
    "    # print(mean_iou_scores, accuracy)\n",
    "    print(f\"=========> Loss at epoch {epoch} is {np.mean(losses)}\")\n",
    "    print(f\"=========> IoU at epoch {epoch} is {np.mean(mean_iou_scores)}\")\n",
    "    print(f\"=========> Pixel acc at epoch {epoch} is {np.mean(accuracy)}\")\n",
    "\n",
    "    model_.train() #TURNING THE TRAIN MODE BACK ON TO ENABLE BATCHNORM/DROPOUT!!\n",
    "\n",
    "    return np.mean(losses), np.mean(mean_iou_scores), np.mean(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "loHUcdGAb_Xy"
   },
   "source": [
    "# SSL models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "num_scales = 3\n",
    "scale_factor = 2\n",
    "\n",
    "# Define the network architecture with feature pyramid\n",
    "class CompletionModel(nn.Module):\n",
    "    def __init__(self, num_scales=3):\n",
    "        super(CompletionModel, self).__init__()\n",
    "        self.num_scales = num_scales\n",
    "\n",
    "        # Define layers for each scale in the feature pyramid\n",
    "        self.encoders = nn.ModuleList()\n",
    "        self.decoders = nn.ModuleList()\n",
    "        self.channel_reducers = nn.ModuleList()  # 1x1 convolution layers to reduce channels\n",
    "#         self.downsamplers = nn.ModuleList()\n",
    "#         self.upsamplers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_scales):\n",
    "            encoder = nn.Sequential(\n",
    "                nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            self.encoders.append(encoder)\n",
    "\n",
    "            decoder = nn.Sequential(\n",
    "                nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose2d(64, 3, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            self.decoders.append(decoder)\n",
    "            \n",
    "            channel_reducer = nn.Conv2d(3, 512, kernel_size=1)  # 1x1 convolution layer to reduce channels\n",
    "            self.channel_reducers.append(channel_reducer)\n",
    "            \n",
    "        self.upsampler = nn.Upsample(scale_factor=scale_factor, mode='bilinear', align_corners=False)\n",
    "        self.downsampler = nn.Upsample(scale_factor=1/scale_factor, mode='bilinear', align_corners=False)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize list to store features from each scale\n",
    "        features = []\n",
    "        #print(f\"x.shape: {x.shape}\")\n",
    "\n",
    "        # Forward pass through each scale in the feature pyramid\n",
    "        for i in range(self.num_scales):\n",
    "            encoder_output = self.encoders[i](x)\n",
    "            features.append(encoder_output)\n",
    "            #print(f\"Feature size at {i}th scale: {features[-1].shape}\")\n",
    "            x = self.downsampler(x)\n",
    "\n",
    "        # Decode the concatenated features\n",
    "        shape_feats = list(features[-1].shape)\n",
    "        shape_feats[1] = 3\n",
    "        prev_output = None#torch.zeros(shape_feats).to(device)\n",
    "        \n",
    "        for i in range(self.num_scales):\n",
    "            decoder_output = self.decoders[i](features[self.num_scales - i - 1])\n",
    "            #print(f\"decoder output shape: {decoder_output.shape}\")\n",
    "            if prev_output is None:\n",
    "                prev_output = self.upsampler(decoder_output)\n",
    "                #print(f\"prev output is none: prev_output.shape : {prev_output.shape}\")\n",
    "            else:\n",
    "                #prev_output = self.channel_reducers[i](prev_output)\n",
    "                #print(f\"prev_output.shape : {prev_output.shape}\")\n",
    "                prev_output = self.upsampler(prev_output+decoder_output)\n",
    "        \n",
    "        #print(decoder_output.shape)\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self supervised training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "image_size = 28\n",
    "\n",
    "# Gaussian Pyramid Constants\n",
    "num_scales = 3\n",
    "scale_factor = 2\n",
    "batch_size = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssl_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop((128, 128)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(*mean_std)\n",
    "])\n",
    "\n",
    "target_transform = transforms.Compose([\n",
    "    MaskToTensor()\n",
    "])\n",
    "\n",
    "ssl_dataset_train = VOC('train', transform=ssl_transform, target_transform=target_transform)\n",
    "ssl_dataset_val = VOC('val', transform=ssl_transform, target_transform=target_transform)\n",
    "ssl_dataset_test = VOC('test', transform=ssl_transform, target_transform=target_transform)\n",
    "\n",
    "combined_dataset = torch.utils.data.ConcatDataset([ssl_dataset_train, ssl_dataset_val, ssl_dataset_test])\n",
    "ssl_dataloader = DataLoader(dataset=combined_dataset, batch_size= 16, shuffle=True, num_workers=NUM_WORKERS, prefetch_factor=PREFETCH_FACTOR, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 1.3800\n",
      "Epoch [2/30], Loss: 1.2426\n",
      "Epoch [3/30], Loss: 1.2490\n",
      "Epoch [4/30], Loss: 1.2191\n",
      "Epoch [5/30], Loss: 1.1839\n",
      "Epoch [6/30], Loss: 1.1557\n",
      "Epoch [7/30], Loss: 1.1423\n",
      "Epoch [8/30], Loss: 1.1259\n",
      "Epoch [9/30], Loss: 1.1590\n",
      "Epoch [10/30], Loss: 1.1845\n",
      "Epoch [11/30], Loss: 1.1655\n",
      "Epoch [12/30], Loss: 1.1335\n",
      "Epoch [13/30], Loss: 1.1673\n",
      "Epoch [14/30], Loss: 1.1518\n",
      "Epoch [15/30], Loss: 1.1407\n",
      "Epoch [16/30], Loss: 1.0863\n",
      "Epoch [17/30], Loss: 1.1356\n",
      "Epoch [18/30], Loss: 1.1304\n",
      "Epoch [19/30], Loss: 1.1370\n",
      "Epoch [20/30], Loss: 1.1224\n",
      "Epoch [21/30], Loss: 1.1407\n",
      "Epoch [22/30], Loss: 1.1168\n",
      "Epoch [23/30], Loss: 1.1389\n",
      "Epoch [24/30], Loss: 1.1251\n",
      "Epoch [25/30], Loss: 1.1539\n",
      "Epoch [26/30], Loss: 1.1503\n",
      "Epoch [27/30], Loss: 1.1593\n",
      "Epoch [28/30], Loss: 1.1272\n",
      "Epoch [29/30], Loss: 1.1852\n",
      "Epoch [30/30], Loss: 1.1173\n"
     ]
    }
   ],
   "source": [
    "model = CompletionModel().to(device)\n",
    "# model.load_state_dict(torch.load('ssl_model.pth'))\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, _ in ssl_dataloader:\n",
    "        images = images.to(device)#.unsqueeze(0)\n",
    "        #print(images.shape)\n",
    "        \n",
    "        occluded_image = images.clone()\n",
    "        _,_, h, w = occluded_image.shape\n",
    "        x = torch.randint(0, w // 2, (1,))\n",
    "        y = torch.randint(0, h // 2, (1,))\n",
    "        occluded_image[:, :, y:y + h // 2, x:x + w // 2] = 0\n",
    "        # Forward pass and loss calculation\n",
    "        completion_images = model(occluded_image.to(device))\n",
    "        loss = criterion(completion_images[:, :, y:y + h // 2, x:x + w // 2], images[:, :, y:y + h // 2, x:x + w // 2])\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Compute the average loss for the epoch\n",
    "    average_loss = running_loss / len(ssl_dataloader)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}\")\n",
    "\n",
    "# Generate completions for test images\n",
    "model.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRnApfp4E5ok"
   },
   "source": [
    "## SSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "xkWyigy-PQhI"
   },
   "outputs": [],
   "source": [
    "class Backbone(nn.Module):\n",
    "    def __init__(self, out_dim):\n",
    "        super(Backbone, self).__init__()\n",
    "        \n",
    "        # Define the layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Define the fully connected (linear) layer\n",
    "        self.fc = nn.Linear(in_features=1568, out_features=out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Perform forward pass\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "_1Zwqss-PIJl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class UNet_ssl(nn.Module):\n",
    "    def __init__(self, n_class=21, n_dim=512):\n",
    "        super(UNet_ssl, self).__init__()\n",
    "\n",
    "        # Encoder (Based on the provided SSL architecture)\n",
    "        #self.encoder = Backbone(out_dim=n_dim)\n",
    "        self.encoder = nn.Sequential(\n",
    "                nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
    "                nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder4 = self.expanding_block(n_dim, 256)\n",
    "        self.decoder3 = self.expanding_block(256, 128)\n",
    "        self.decoder2 = self.expanding_block(128, 64)\n",
    "        self.decoder1 = self.expanding_block(64, 32)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output = nn.Conv2d(32, n_class, kernel_size=1)\n",
    "        \n",
    "        # Initialize the weights\n",
    "        self.initialize_weights()\n",
    "        \n",
    "    def expanding_block(self, in_channels, out_channels):\n",
    "        block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ConvTranspose2d(out_channels, in_channels // 2, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(in_channels // 2)\n",
    "        )\n",
    "        return block\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder (SSL model)\n",
    "        features = self.encoder(x)\n",
    "        \n",
    "        # Decoder\n",
    "        decode4 = self.decoder4(features)\n",
    "        decode3 = self.decoder3(decode4)\n",
    "        decode2 = self.decoder2(decode3)\n",
    "        decode1 = self.decoder1(decode2)\n",
    "        \n",
    "        # Output\n",
    "        output = self.output(decode1)\n",
    "        #print(output.shape, decode1.shape, decode2.shape, decode3.shape, decode4.shape)\n",
    "#         torch.Size([16, 21, 224, 224]) \n",
    "#         torch.Size([16, 32, 224, 224]) \n",
    "#         torch.Size([16, 64, 112, 112]) \n",
    "#         torch.Size([16, 128, 56, 56]) \n",
    "#         torch.Size([16, 256, 28, 28])\n",
    "        return output\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "                nn.init.kaiming_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with SSL Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "ZQONHMeGPIPO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2983412/3566168316.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  trainOutputs = F.softmax(trainOutputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> epoch0, iter0, Train set=> loss: 3.0455620288848877, IOU: 0.009777494286819776, Acc: 0.0507965087890625\n",
      "==> epoch0, iter10, Train set=> loss: 2.9737496376037598, IOU: 0.020023597635912074, Acc: 0.20377731323242188\n",
      "Finish epoch 0, time elapsed 13.72953462600708\n",
      "=========> Loss at epoch 0 is 2.9909465312957764\n",
      "=========> IoU at epoch 0 is 0.02083676760988581\n",
      "=========> Pixel acc at epoch 0 is 0.2467904772077288\n",
      "==> epoch1, iter0, Train set=> loss: 2.9183528423309326, IOU: 0.023454618179995685, Acc: 0.2667198181152344\n",
      "==> epoch1, iter10, Train set=> loss: 2.8511500358581543, IOU: 0.02526763056529278, Acc: 0.3138999938964844\n",
      "Finish epoch 1, time elapsed 12.877024173736572\n",
      "=========> Loss at epoch 1 is 2.877039040837969\n",
      "=========> IoU at epoch 1 is 0.022855239341463877\n",
      "=========> Pixel acc at epoch 1 is 0.2928738730294364\n",
      "==> epoch2, iter0, Train set=> loss: 2.8301379680633545, IOU: 0.027419831711188985, Acc: 0.3246650695800781\n",
      "==> epoch2, iter10, Train set=> loss: 2.7503416538238525, IOU: 0.0315133456090694, Acc: 0.4006996154785156\n",
      "Finish epoch 2, time elapsed 13.375523090362549\n",
      "=========> Loss at epoch 2 is 2.765583804675511\n",
      "=========> IoU at epoch 2 is 0.027225763067342586\n",
      "=========> Pixel acc at epoch 2 is 0.3978655678885324\n",
      "==> epoch3, iter0, Train set=> loss: 2.75040864944458, IOU: 0.028498703923366703, Acc: 0.39484405517578125\n",
      "==> epoch3, iter10, Train set=> loss: 2.751222610473633, IOU: 0.0287577276897309, Acc: 0.383758544921875\n",
      "Finish epoch 3, time elapsed 12.807514667510986\n",
      "=========> Loss at epoch 3 is 2.7434576068605696\n",
      "=========> IoU at epoch 3 is 0.027046880390809745\n",
      "=========> Pixel acc at epoch 3 is 0.3968795231410435\n",
      "==> epoch4, iter0, Train set=> loss: 2.7373831272125244, IOU: 0.02891963870190845, Acc: 0.3989715576171875\n",
      "==> epoch4, iter10, Train set=> loss: 2.7164463996887207, IOU: 0.029779889393492215, Acc: 0.4124183654785156\n",
      "Finish epoch 4, time elapsed 13.589052200317383\n",
      "=========> Loss at epoch 4 is 2.7674080474036082\n",
      "=========> IoU at epoch 4 is 0.026360227729164323\n",
      "=========> Pixel acc at epoch 4 is 0.3613741193498884\n",
      "==> epoch5, iter0, Train set=> loss: 2.6828625202178955, IOU: 0.03242974570333035, Acc: 0.4463615417480469\n",
      "==> epoch5, iter10, Train set=> loss: 2.692340612411499, IOU: 0.029887649664264108, Acc: 0.4333038330078125\n",
      "Finish epoch 5, time elapsed 13.075931310653687\n",
      "=========> Loss at epoch 5 is 2.6986210516520908\n",
      "=========> IoU at epoch 5 is 0.027605831303570093\n",
      "=========> Pixel acc at epoch 5 is 0.42353570120675227\n",
      "==> epoch6, iter0, Train set=> loss: 2.7314188480377197, IOU: 0.027911096904254824, Acc: 0.389434814453125\n",
      "==> epoch6, iter10, Train set=> loss: 2.6779861450195312, IOU: 0.027946296798133593, Acc: 0.4442291259765625\n",
      "Finish epoch 6, time elapsed 12.775650978088379\n",
      "=========> Loss at epoch 6 is 2.6938944714409963\n",
      "=========> IoU at epoch 6 is 0.028762063273216065\n",
      "=========> Pixel acc at epoch 6 is 0.4269038609095982\n",
      "==> epoch7, iter0, Train set=> loss: 2.6560215950012207, IOU: 0.03182359220193563, Acc: 0.4667701721191406\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(unet_ssl_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0005\u001b[39m)\u001b[38;5;66;03m#  choose an optimizer\u001b[39;00m\n\u001b[1;32m     22\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m---> 23\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43munet_ssl_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 37\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     34\u001b[0m inputs \u001b[38;5;241m=\u001b[39m  inputs\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;66;03m#  transfer the input to the same device as the model's\u001b[39;00m\n\u001b[1;32m     35\u001b[0m labels \u001b[38;5;241m=\u001b[39m   labels\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;66;03m#  transfer the labels to the same device as the model's\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m trainOutputs \u001b[38;5;241m=\u001b[39m  \u001b[43mmodel_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#   Compute outputs. we will not need to transfer the output, it will be automatically in the same device as the model's!\u001b[39;00m\n\u001b[1;32m     38\u001b[0m trainOutputs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(trainOutputs)\n\u001b[1;32m     39\u001b[0m loss \u001b[38;5;241m=\u001b[39m  criterion(trainOutputs,labels)  \u001b[38;5;66;03m#  calculate loss\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[62], line 49\u001b[0m, in \u001b[0;36mUNet_ssl.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# Encoder (SSL model)\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# Decoder\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     decode4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder4(features)\n",
      "File \u001b[0;32m/mnt/sphere/home/asagrawal/advancedcv/.env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/sphere/home/asagrawal/advancedcv/.env/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/mnt/sphere/home/asagrawal/advancedcv/.env/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/sphere/home/asagrawal/advancedcv/.env/lib/python3.8/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/sphere/home/asagrawal/advancedcv/.env/lib/python3.8/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/sphere/home/asagrawal/advancedcv/.env/lib/python3.8/site-packages/torch/fx/traceback.py:41\u001b[0m, in \u001b[0;36mformat_stack\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [current_meta\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstack_trace\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# fallback to traceback.format_stack()\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m traceback\u001b[38;5;241m.\u001b[39mformat_list(\u001b[43mtraceback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m/usr/lib/python3.8/traceback.py:211\u001b[0m, in \u001b[0;36mextract_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m     f \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe()\u001b[38;5;241m.\u001b[39mf_back\n\u001b[0;32m--> 211\u001b[0m stack \u001b[38;5;241m=\u001b[39m \u001b[43mStackSummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwalk_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m stack\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stack\n",
      "File \u001b[0;32m/usr/lib/python3.8/traceback.py:362\u001b[0m, in \u001b[0;36mStackSummary.extract\u001b[0;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[1;32m    359\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(FrameSummary(\n\u001b[1;32m    360\u001b[0m         filename, lineno, name, lookup_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39mf_locals))\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m fnames:\n\u001b[0;32m--> 362\u001b[0m     \u001b[43mlinecache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# If immediate lookup was desired, trigger lookups now.\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lookup_lines:\n",
      "File \u001b[0;32m/usr/lib/python3.8/linecache.py:74\u001b[0m, in \u001b[0;36mcheckcache\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m   \u001b[38;5;66;03m# no-op for files loaded via a __loader__\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 74\u001b[0m     stat \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     cache\u001b[38;5;241m.\u001b[39mpop(filename, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create the U-Net model\n",
    "\n",
    "unet_ssl_model = UNet_ssl(n_class=21, n_dim=512)  # Assuming 21 classes for segmentation\n",
    "\n",
    "# Load the SSL model weights\n",
    "# ssl_model_weights = torch.load('my_ssl.pth', map_location=torch.device(device))\n",
    "# unet_ssl_model.encoder.load_state_dict(ssl_model_weights)\n",
    "unet_ssl_model.encoder = model.encoders[0]\n",
    "for param in unet_ssl_model.encoder.parameters():\n",
    "    param.required_grad = True\n",
    "\n",
    "# Move the U-Net model to the desired device (e.g., GPU)\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "unet_ssl_model = unet_ssl_model.to(device)\n",
    "\n",
    "# # Print the U-Net model architecture\n",
    "# print(unet_ssl_model)\n",
    "\n",
    "epochs = 10\n",
    "from torch import optim\n",
    "optimizer = optim.Adam(unet_ssl_model.parameters(), lr=0.0005)#  choose an optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "train(unet_ssl_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "id": "GbCVDRxGPIRF",
    "outputId": "bcff3e81-c437-45f8-a6fa-5ca8a16ab5b2"
   },
   "source": [
    "## Training without SSL Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "byUEBl3EQehW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2983412/3566168316.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  trainOutputs = F.softmax(trainOutputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> epoch0, iter0, Train set=> loss: 3.0469515323638916, IOU: 0.010278444196282484, Acc: 0.04852294921875\n",
      "==> epoch0, iter10, Train set=> loss: 2.6845858097076416, IOU: 0.03140308507293698, Acc: 0.4603118896484375\n",
      "Finish epoch 0, time elapsed 13.980654954910278\n",
      "=========> Loss at epoch 0 is 3.0285376140049527\n",
      "=========> IoU at epoch 0 is 0.008648442468765628\n",
      "=========> Pixel acc at epoch 0 is 0.10137056623186383\n",
      "==> epoch1, iter0, Train set=> loss: 2.6619133949279785, IOU: 0.037622107008029274, Acc: 0.4728736877441406\n",
      "==> epoch1, iter10, Train set=> loss: 2.6263508796691895, IOU: 0.033360621944602764, Acc: 0.5036048889160156\n",
      "Finish epoch 1, time elapsed 12.887197494506836\n",
      "=========> Loss at epoch 1 is 2.560858130455017\n",
      "=========> IoU at epoch 1 is 0.037264220688280524\n",
      "=========> Pixel acc at epoch 1 is 0.5630130767822266\n",
      "==> epoch2, iter0, Train set=> loss: 2.5642144680023193, IOU: 0.047152207605744435, Acc: 0.5713348388671875\n",
      "==> epoch2, iter10, Train set=> loss: 2.4925525188446045, IOU: 0.03968236456400622, Acc: 0.6379852294921875\n",
      "Finish epoch 2, time elapsed 12.632883548736572\n",
      "=========> Loss at epoch 2 is 2.4741429771695818\n",
      "=========> IoU at epoch 2 is 0.04072740687952188\n",
      "=========> Pixel acc at epoch 2 is 0.6519990103585379\n",
      "==> epoch3, iter0, Train set=> loss: 2.4999136924743652, IOU: 0.03833090176840933, Acc: 0.6278839111328125\n",
      "==> epoch3, iter10, Train set=> loss: 2.477879524230957, IOU: 0.0429258271491157, Acc: 0.6472969055175781\n",
      "Finish epoch 3, time elapsed 13.075373888015747\n",
      "=========> Loss at epoch 3 is 2.4602999005998885\n",
      "=========> IoU at epoch 3 is 0.05149614944121812\n",
      "=========> Pixel acc at epoch 3 is 0.6652350834437779\n",
      "==> epoch4, iter0, Train set=> loss: 2.4725441932678223, IOU: 0.049839922490843444, Acc: 0.6538238525390625\n",
      "==> epoch4, iter10, Train set=> loss: 2.5065202713012695, IOU: 0.050247529440947146, Acc: 0.6201095581054688\n",
      "Finish epoch 4, time elapsed 12.661881446838379\n",
      "=========> Loss at epoch 4 is 2.433969804218837\n",
      "=========> IoU at epoch 4 is 0.05220957061467995\n",
      "=========> Pixel acc at epoch 4 is 0.6904523577008929\n",
      "==> epoch5, iter0, Train set=> loss: 2.414942741394043, IOU: 0.04201048533560453, Acc: 0.7146453857421875\n",
      "==> epoch5, iter10, Train set=> loss: 2.4727325439453125, IOU: 0.04437694257863976, Acc: 0.6568336486816406\n",
      "Finish epoch 5, time elapsed 13.010676622390747\n",
      "=========> Loss at epoch 5 is 2.641212684767587\n",
      "=========> IoU at epoch 5 is 0.033621820089595765\n",
      "=========> Pixel acc at epoch 5 is 0.48497434343610496\n",
      "==> epoch6, iter0, Train set=> loss: 2.380979537963867, IOU: 0.04383724885752306, Acc: 0.7528114318847656\n",
      "==> epoch6, iter10, Train set=> loss: 2.453371047973633, IOU: 0.041612687013664565, Acc: 0.6730117797851562\n",
      "Finish epoch 6, time elapsed 12.685227632522583\n",
      "=========> Loss at epoch 6 is 2.379936064992632\n",
      "=========> IoU at epoch 6 is 0.04985167443433366\n",
      "=========> Pixel acc at epoch 6 is 0.7442895616803851\n",
      "==> epoch7, iter0, Train set=> loss: 2.3941383361816406, IOU: 0.04273058547952033, Acc: 0.7335853576660156\n",
      "==> epoch7, iter10, Train set=> loss: 2.392582654953003, IOU: 0.038189313397049826, Acc: 0.733612060546875\n",
      "Finish epoch 7, time elapsed 13.25406551361084\n",
      "=========> Loss at epoch 7 is 2.3737137658255443\n",
      "=========> IoU at epoch 7 is 0.05108833449598261\n",
      "=========> Pixel acc at epoch 7 is 0.7500661032540458\n",
      "==> epoch8, iter0, Train set=> loss: 2.400184154510498, IOU: 0.03500406490918023, Acc: 0.7254219055175781\n",
      "==> epoch8, iter10, Train set=> loss: 2.3889737129211426, IOU: 0.042484405272388705, Acc: 0.735992431640625\n",
      "Finish epoch 8, time elapsed 12.89108395576477\n",
      "=========> Loss at epoch 8 is 2.3750013964516774\n",
      "=========> IoU at epoch 8 is 0.05357796711875562\n",
      "=========> Pixel acc at epoch 8 is 0.7506091526576452\n",
      "==> epoch9, iter0, Train set=> loss: 2.4029855728149414, IOU: 0.03863060159046459, Acc: 0.7221031188964844\n",
      "==> epoch9, iter10, Train set=> loss: 2.4993677139282227, IOU: 0.03504708301110135, Acc: 0.6244392395019531\n",
      "Finish epoch 9, time elapsed 12.88979434967041\n",
      "=========> Loss at epoch 9 is 2.389230864388602\n",
      "=========> IoU at epoch 9 is 0.046979959735552565\n",
      "=========> Pixel acc at epoch 9 is 0.7346084049769811\n"
     ]
    }
   ],
   "source": [
    "# Create the U-Net model\n",
    "\n",
    "unet_model = UNet_ssl(n_class=21, n_dim=512)  # Assuming 21 classes for segmentation\n",
    "unet_model = unet_model.to(device)\n",
    "optimizer = optim.Adam(unet_model.parameters(), lr=0.005)#  choose an optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "epochs = 10\n",
    "train(unet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 7.1478,  7.3285,  7.0590,  ...,  7.2983,  7.3841,  7.7244],\n",
       "          [ 7.1706,  7.5369,  7.1596,  ...,  7.4426,  7.2236,  7.6123],\n",
       "          [ 7.3243,  7.4142,  7.0578,  ...,  6.9470,  7.0845,  7.2374],\n",
       "          ...,\n",
       "          [ 7.3643,  7.4369,  7.3529,  ...,  7.6335,  7.5181,  7.8006],\n",
       "          [ 7.5092,  7.8373,  7.6364,  ...,  7.5906,  7.4520,  7.6561],\n",
       "          [ 7.3679,  7.8861,  7.3679,  ...,  7.6909,  7.4561,  7.7565]],\n",
       "\n",
       "         [[-3.4186, -2.6196, -3.2138,  ..., -3.0133, -3.4966, -2.7109],\n",
       "          [-2.9764, -2.8986, -2.7083,  ..., -2.9631, -2.9254, -2.9648],\n",
       "          [-3.1498, -2.7724, -2.9378,  ..., -3.1476, -3.0485, -2.7120],\n",
       "          ...,\n",
       "          [-2.9469, -2.9265, -2.5910,  ..., -3.0175, -2.7985, -3.0159],\n",
       "          [-3.3929, -2.6492, -3.1467,  ..., -2.9455, -3.3384, -2.6143],\n",
       "          [-2.9874, -2.8870, -2.8593,  ..., -3.0610, -2.9879, -3.0257]],\n",
       "\n",
       "         [[-2.5269, -3.0030, -2.6166,  ..., -2.9646, -2.4662, -3.0527],\n",
       "          [-3.4777, -3.3505, -3.4025,  ..., -3.4854, -3.4466, -3.4469],\n",
       "          [-2.9237, -3.2163, -3.0955,  ..., -3.1510, -2.7481, -3.1064],\n",
       "          ...,\n",
       "          [-3.5028, -3.2688, -3.4367,  ..., -3.5403, -3.5798, -3.4899],\n",
       "          [-2.5599, -3.1753, -2.8210,  ..., -3.1825, -2.6557, -3.1500],\n",
       "          [-3.4709, -3.4827, -3.4577,  ..., -3.4050, -3.5254, -3.3762]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.7453, -2.7776, -2.8783,  ..., -2.9857, -2.7938, -2.7828],\n",
       "          [-2.9779, -3.0947, -2.9445,  ..., -3.3175, -2.7673, -3.0606],\n",
       "          [-2.8893, -3.1140, -2.9441,  ..., -3.2966, -2.7803, -2.8991],\n",
       "          ...,\n",
       "          [-3.2490, -3.3000, -3.1726,  ..., -3.7626, -3.0673, -3.4405],\n",
       "          [-2.7374, -2.8825, -2.8235,  ..., -3.0343, -2.7676, -2.8081],\n",
       "          [-2.8633, -3.2702, -3.0945,  ..., -3.4541, -3.0458, -3.1926]],\n",
       "\n",
       "         [[-3.1296, -2.9645, -2.9871,  ..., -3.0954, -3.1225, -3.1405],\n",
       "          [-3.6517, -3.1516, -3.7310,  ..., -3.1487, -3.7288, -3.2116],\n",
       "          [-3.2322, -2.9821, -3.0807,  ..., -3.0332, -3.1105, -3.0813],\n",
       "          ...,\n",
       "          [-3.4886, -3.2257, -3.5014,  ..., -3.2203, -3.7046, -3.2678],\n",
       "          [-3.2920, -3.1216, -3.2564,  ..., -3.2121, -3.2393, -3.1611],\n",
       "          [-3.5349, -3.2922, -3.4927,  ..., -3.3273, -3.5949, -3.2826]],\n",
       "\n",
       "         [[-2.8726, -3.2147, -3.0383,  ..., -2.9927, -3.1649, -3.2870],\n",
       "          [-3.4979, -2.9159, -3.3465,  ..., -3.0163, -3.5417, -2.9324],\n",
       "          [-2.6932, -3.2221, -2.6733,  ..., -2.8060, -2.8980, -3.2022],\n",
       "          ...,\n",
       "          [-3.1304, -2.9371, -2.9251,  ..., -3.2443, -3.3976, -3.0871],\n",
       "          [-2.9742, -3.4611, -2.8725,  ..., -3.1750, -2.9613, -3.3962],\n",
       "          [-3.4085, -3.0521, -3.0908,  ..., -2.9908, -3.3915, -2.9039]]],\n",
       "\n",
       "\n",
       "        [[[ 7.1379,  7.3137,  7.0755,  ...,  7.2891,  7.3820,  7.7197],\n",
       "          [ 7.1695,  7.5382,  7.1735,  ...,  7.4385,  7.2239,  7.6109],\n",
       "          [ 7.3541,  7.4389,  7.0486,  ...,  6.9575,  7.0922,  7.2483],\n",
       "          ...,\n",
       "          [ 7.3571,  7.4290,  7.3458,  ...,  7.4490,  7.5055,  7.8993],\n",
       "          [ 7.5079,  7.8334,  7.6122,  ...,  7.5591,  7.5090,  7.6951],\n",
       "          [ 7.3709,  7.8787,  7.3651,  ...,  7.7166,  7.4618,  7.8656]],\n",
       "\n",
       "         [[-3.4173, -2.6213, -3.2209,  ..., -3.0129, -3.4960, -2.7105],\n",
       "          [-2.9744, -2.8851, -2.6935,  ..., -2.9622, -2.9262, -2.9637],\n",
       "          [-3.1561, -2.7738, -2.9464,  ..., -3.1445, -3.0516, -2.7144],\n",
       "          ...,\n",
       "          [-2.9464, -2.9289, -2.5890,  ..., -3.0987, -2.8366, -3.0985],\n",
       "          [-3.3931, -2.6489, -3.1514,  ..., -2.9349, -3.3151, -2.6183],\n",
       "          [-2.9878, -2.8934, -2.8630,  ..., -3.0363, -2.9920, -3.0204]],\n",
       "\n",
       "         [[-2.5101, -2.9899, -2.6011,  ..., -2.9622, -2.4667, -3.0519],\n",
       "          [-3.4777, -3.3663, -3.4022,  ..., -3.4835, -3.4473, -3.4457],\n",
       "          [-2.9547, -3.2382, -3.1287,  ..., -3.1548, -2.7501, -3.1089],\n",
       "          ...,\n",
       "          [-3.5014, -3.2638, -3.4353,  ..., -3.3686, -3.5408, -3.4808],\n",
       "          [-2.5657, -3.1768, -2.8204,  ..., -3.1624, -2.6695, -3.1664],\n",
       "          [-3.4728, -3.4748, -3.4562,  ..., -3.4385, -3.5238, -3.4143]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.7354, -2.7660, -2.8650,  ..., -2.9835, -2.7936, -2.7825],\n",
       "          [-2.9505, -3.1016, -2.9206,  ..., -3.3162, -2.7693, -3.0608],\n",
       "          [-2.9018, -3.1232, -2.9332,  ..., -3.2986, -2.7846, -2.9028],\n",
       "          ...,\n",
       "          [-3.2499, -3.2944, -3.1616,  ..., -3.6055, -3.0862, -3.4468],\n",
       "          [-2.7425, -2.8843, -2.8246,  ..., -3.0351, -2.7695, -2.8283],\n",
       "          [-2.8750, -3.2639, -3.1047,  ..., -3.4943, -3.0264, -3.2503]],\n",
       "\n",
       "         [[-3.1245, -2.9680, -2.9800,  ..., -3.0933, -3.1225, -3.1383],\n",
       "          [-3.6546, -3.1520, -3.7437,  ..., -3.1469, -3.7291, -3.2108],\n",
       "          [-3.2471, -2.9940, -3.1134,  ..., -3.0369, -3.1129, -3.0834],\n",
       "          ...,\n",
       "          [-3.4948, -3.2181, -3.5095,  ..., -3.1457, -3.7208, -3.2719],\n",
       "          [-3.2894, -3.1194, -3.2533,  ..., -3.2112, -3.2718, -3.1576],\n",
       "          [-3.5387, -3.2900, -3.4943,  ..., -3.2998, -3.5898, -3.3150]],\n",
       "\n",
       "         [[-2.8810, -3.2181, -3.0600,  ..., -2.9902, -3.1615, -3.2855],\n",
       "          [-3.5003, -2.9347, -3.3461,  ..., -3.0143, -3.5421, -2.9318],\n",
       "          [-2.6841, -3.2286, -2.6452,  ..., -2.8098, -2.9017, -3.2048],\n",
       "          ...,\n",
       "          [-3.1374, -2.9305, -2.9321,  ..., -3.1438, -3.3570, -3.0262],\n",
       "          [-2.9716, -3.4565, -2.8651,  ..., -3.1808, -2.9599, -3.4218],\n",
       "          [-3.4104, -3.0426, -3.0917,  ..., -3.0531, -3.3834, -2.9413]]],\n",
       "\n",
       "\n",
       "        [[[ 7.1544,  7.3408,  7.0407,  ...,  7.2973,  7.3821,  7.7274],\n",
       "          [ 7.1673,  7.5449,  7.1555,  ...,  7.4524,  7.2231,  7.6254],\n",
       "          [ 7.3133,  7.3942,  7.0736,  ...,  6.9552,  7.0806,  7.2303],\n",
       "          ...,\n",
       "          [ 7.3634,  7.4217,  7.3583,  ...,  7.4418,  7.5043,  7.8951],\n",
       "          [ 7.5043,  7.8336,  7.6331,  ...,  7.5512,  7.5085,  7.6983],\n",
       "          [ 7.3671,  7.8846,  7.3710,  ...,  7.7131,  7.4620,  7.8726]],\n",
       "\n",
       "         [[-3.4202, -2.6170, -3.2042,  ..., -3.0079, -3.4944, -2.7072],\n",
       "          [-2.9806, -2.9080, -2.7211,  ..., -2.9634, -2.9276, -2.9670],\n",
       "          [-3.1452, -2.7679, -2.9422,  ..., -3.1465, -3.0490, -2.7104],\n",
       "          ...,\n",
       "          [-2.9449, -2.9264, -2.5890,  ..., -3.1020, -2.8396, -3.0987],\n",
       "          [-3.3932, -2.6425, -3.1439,  ..., -2.9344, -3.3148, -2.6211],\n",
       "          [-2.9896, -2.8882, -2.8619,  ..., -3.0362, -2.9930, -3.0214]],\n",
       "\n",
       "         [[-2.5320, -3.0094, -2.6136,  ..., -2.9653, -2.4657, -3.0546],\n",
       "          [-3.4762, -3.3460, -3.4025,  ..., -3.4871, -3.4463, -3.4502],\n",
       "          [-2.9166, -3.2072, -3.0962,  ..., -3.1525, -2.7426, -3.1011],\n",
       "          ...,\n",
       "          [-3.5032, -3.2645, -3.4371,  ..., -3.3611, -3.5400, -3.4786],\n",
       "          [-2.5620, -3.1767, -2.8246,  ..., -3.1601, -2.6682, -3.1674],\n",
       "          [-3.4725, -3.4780, -3.4611,  ..., -3.4373, -3.5227, -3.4171]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.7509, -2.7800, -2.8789,  ..., -2.9841, -2.7918, -2.7825],\n",
       "          [-2.9852, -3.0935, -2.9436,  ..., -3.3209, -2.7676, -3.0656],\n",
       "          [-2.8843, -3.1024, -2.9503,  ..., -3.3011, -2.7759, -2.8929],\n",
       "          ...,\n",
       "          [-3.2493, -3.2949, -3.1594,  ..., -3.5977, -3.0855, -3.4415],\n",
       "          [-2.7362, -2.8825, -2.8242,  ..., -3.0350, -2.7695, -2.8310],\n",
       "          [-2.8717, -3.2663, -3.1023,  ..., -3.4911, -3.0240, -3.2517]],\n",
       "\n",
       "         [[-3.1316, -2.9663, -2.9836,  ..., -3.0939, -3.1250, -3.1412],\n",
       "          [-3.6536, -3.1511, -3.7343,  ..., -3.1494, -3.7264, -3.2135],\n",
       "          [-3.2274, -2.9807, -3.0776,  ..., -3.0316, -3.1091, -3.0808],\n",
       "          ...,\n",
       "          [-3.4939, -3.2245, -3.5122,  ..., -3.1416, -3.7235, -3.2692],\n",
       "          [-3.2927, -3.1178, -3.2546,  ..., -3.2074, -3.2728, -3.1565],\n",
       "          [-3.5309, -3.2907, -3.4921,  ..., -3.2972, -3.5909, -3.3181]],\n",
       "\n",
       "         [[-2.8700, -3.2124, -3.0288,  ..., -2.9950, -3.1640, -3.2904],\n",
       "          [-3.4971, -2.9089, -3.3455,  ..., -3.0197, -3.5403, -2.9359],\n",
       "          [-2.6872, -3.2125, -2.6709,  ..., -2.8107, -2.8990, -3.2000],\n",
       "          ...,\n",
       "          [-3.1308, -2.9298, -2.9271,  ..., -3.1368, -3.3588, -3.0206],\n",
       "          [-2.9690, -3.4639, -2.8664,  ..., -3.1787, -2.9626, -3.4224],\n",
       "          [-3.4099, -3.0490, -3.0924,  ..., -3.0501, -3.3828, -2.9430]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 7.1582,  7.3474,  7.0327,  ...,  7.3016,  7.3831,  7.7271],\n",
       "          [ 7.1598,  7.5453,  7.1498,  ...,  7.4501,  7.2233,  7.6204],\n",
       "          [ 7.3046,  7.3794,  7.0640,  ...,  6.9457,  7.0798,  7.2294],\n",
       "          ...,\n",
       "          [ 7.3625,  7.4474,  7.3431,  ...,  7.4401,  7.5042,  7.8950],\n",
       "          [ 7.5097,  7.8376,  7.6262,  ...,  7.5514,  7.5081,  7.6983],\n",
       "          [ 7.3689,  7.8850,  7.3650,  ...,  7.7127,  7.4623,  7.8730]],\n",
       "\n",
       "         [[-3.4229, -2.6159, -3.1945,  ..., -3.0105, -3.4954, -2.7088],\n",
       "          [-2.9876, -2.9151, -2.7329,  ..., -2.9640, -2.9264, -2.9668],\n",
       "          [-3.1384, -2.7624, -2.9375,  ..., -3.1481, -3.0476, -2.7103],\n",
       "          ...,\n",
       "          [-2.9461, -2.9258, -2.5914,  ..., -3.1023, -2.8395, -3.0983],\n",
       "          [-3.3928, -2.6518, -3.1496,  ..., -2.9343, -3.3147, -2.6209],\n",
       "          [-2.9872, -2.8900, -2.8592,  ..., -3.0360, -2.9934, -3.0216]],\n",
       "\n",
       "         [[-2.5329, -3.0119, -2.6092,  ..., -2.9660, -2.4659, -3.0539],\n",
       "          [-3.4733, -3.3419, -3.3996,  ..., -3.4871, -3.4463, -3.4491],\n",
       "          [-2.9102, -3.2009, -3.0886,  ..., -3.1497, -2.7441, -3.1023],\n",
       "          ...,\n",
       "          [-3.5016, -3.2738, -3.4348,  ..., -3.3603, -3.5400, -3.4789],\n",
       "          [-2.5614, -3.1758, -2.8185,  ..., -3.1602, -2.6683, -3.1677],\n",
       "          [-3.4706, -3.4812, -3.4548,  ..., -3.4371, -3.5228, -3.4169]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.7532, -2.7769, -2.8750,  ..., -2.9860, -2.7927, -2.7826],\n",
       "          [-2.9855, -3.0911, -2.9363,  ..., -3.3199, -2.7674, -3.0633],\n",
       "          [-2.8789, -3.0906, -2.9429,  ..., -3.2980, -2.7761, -2.8939],\n",
       "          ...,\n",
       "          [-3.2445, -3.3046, -3.1751,  ..., -3.5969, -3.0852, -3.4420],\n",
       "          [-2.7401, -2.8835, -2.8236,  ..., -3.0349, -2.7696, -2.8312],\n",
       "          [-2.8654, -3.2696, -3.0944,  ..., -3.4913, -3.0244, -3.2517]],\n",
       "\n",
       "         [[-3.1321, -2.9710, -2.9802,  ..., -3.0952, -3.1239, -3.1415],\n",
       "          [-3.6556, -3.1500, -3.7371,  ..., -3.1499, -3.7274, -3.2130],\n",
       "          [-3.2228, -2.9840, -3.0711,  ..., -3.0303, -3.1088, -3.0804],\n",
       "          ...,\n",
       "          [-3.4911, -3.2235, -3.4978,  ..., -3.1418, -3.7236, -3.2689],\n",
       "          [-3.2909, -3.1222, -3.2558,  ..., -3.2078, -3.2728, -3.1562],\n",
       "          [-3.5380, -3.2919, -3.4942,  ..., -3.2970, -3.5908, -3.3181]],\n",
       "\n",
       "         [[-2.8669, -3.2074, -3.0213,  ..., -2.9951, -3.1656, -3.2892],\n",
       "          [-3.4914, -2.9032, -3.3336,  ..., -3.0188, -3.5408, -2.9345],\n",
       "          [-2.6819, -3.2059, -2.6563,  ..., -2.8067, -2.8974, -3.1997],\n",
       "          ...,\n",
       "          [-3.1348, -2.9427, -2.9265,  ..., -3.1366, -3.3587, -3.0211],\n",
       "          [-2.9746, -3.4576, -2.8735,  ..., -3.1788, -2.9622, -3.4226],\n",
       "          [-3.4080, -3.0492, -3.0900,  ..., -3.0504, -3.3827, -2.9430]]],\n",
       "\n",
       "\n",
       "        [[[ 7.1546,  7.3408,  7.0411,  ...,  7.2879,  7.3808,  7.7132],\n",
       "          [ 7.1672,  7.5448,  7.1555,  ...,  7.4274,  7.2238,  7.5954],\n",
       "          [ 7.3129,  7.3938,  7.0733,  ...,  6.9552,  7.1022,  7.2615],\n",
       "          ...,\n",
       "          [ 7.3567,  7.3883,  7.3350,  ...,  7.5647,  7.4947,  7.8823],\n",
       "          [ 7.4865,  7.8122,  7.5913,  ...,  7.6017,  7.4995,  7.6801],\n",
       "          [ 7.3682,  7.8772,  7.3668,  ...,  7.7390,  7.4589,  7.8112]],\n",
       "\n",
       "         [[-3.4202, -2.6169, -3.2041,  ..., -3.0180, -3.4982, -2.7143],\n",
       "          [-2.9806, -2.9080, -2.7212,  ..., -2.9611, -2.9246, -2.9602],\n",
       "          [-3.1451, -2.7679, -2.9420,  ..., -3.1435, -3.0525, -2.7172],\n",
       "          ...,\n",
       "          [-2.9397, -2.9331, -2.5965,  ..., -3.0810, -2.8189, -3.0878],\n",
       "          [-3.3916, -2.6193, -3.1329,  ..., -2.9384, -3.3242, -2.6288],\n",
       "          [-2.9952, -2.8940, -2.8702,  ..., -3.0521, -2.9844, -3.0256]],\n",
       "\n",
       "         [[-2.5320, -3.0094, -2.6136,  ..., -2.9610, -2.4667, -3.0493],\n",
       "          [-3.4762, -3.3459, -3.4025,  ..., -3.4811, -3.4476, -3.4416],\n",
       "          [-2.9163, -3.2069, -3.0960,  ..., -3.1568, -2.7594, -3.1170],\n",
       "          ...,\n",
       "          [-3.5054, -3.2515, -3.4306,  ..., -3.4336, -3.5490, -3.4837],\n",
       "          [-2.5746, -3.1808, -2.8305,  ..., -3.1777, -2.6654, -3.1548],\n",
       "          [-3.4810, -3.4581, -3.4676,  ..., -3.4370, -3.5216, -3.3998]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.7508, -2.7799, -2.8789,  ..., -2.9845, -2.7952, -2.7829],\n",
       "          [-2.9853, -3.0935, -2.9435,  ..., -3.3126, -2.7696, -3.0557],\n",
       "          [-2.8841, -3.1022, -2.9500,  ..., -3.2949, -2.7919, -2.9122],\n",
       "          ...,\n",
       "          [-3.2552, -3.2824, -3.1275,  ..., -3.6644, -3.0890, -3.4386],\n",
       "          [-2.7413, -2.8860, -2.8290,  ..., -3.0484, -2.7674, -2.8103],\n",
       "          [-2.9077, -3.2540, -3.1250,  ..., -3.4849, -3.0329, -3.2298]],\n",
       "\n",
       "         [[-3.1317, -2.9662, -2.9837,  ..., -3.0943, -3.1193, -3.1360],\n",
       "          [-3.6535, -3.1512, -3.7343,  ..., -3.1452, -3.7314, -3.2076],\n",
       "          [-3.2273, -2.9806, -3.0775,  ..., -3.0412, -3.1172, -3.0840],\n",
       "          ...,\n",
       "          [-3.5154, -3.2112, -3.5414,  ..., -3.1673, -3.7141, -3.2841],\n",
       "          [-3.2881, -3.1001, -3.2395,  ..., -3.2076, -3.2624, -3.1705],\n",
       "          [-3.5260, -3.2782, -3.4984,  ..., -3.3131, -3.5966, -3.3065]],\n",
       "\n",
       "         [[-2.8699, -3.2124, -3.0288,  ..., -2.9874, -3.1599, -3.2808],\n",
       "          [-3.4970, -2.9089, -3.3454,  ..., -3.0103, -3.5433, -2.9282],\n",
       "          [-2.6872, -3.2125, -2.6709,  ..., -2.8063, -2.9006, -3.2088],\n",
       "          ...,\n",
       "          [-3.1432, -2.9087, -2.9393,  ..., -3.1727, -3.3721, -3.0361],\n",
       "          [-2.9448, -3.4672, -2.8382,  ..., -3.1827, -2.9671, -3.3956],\n",
       "          [-3.4228, -3.0363, -3.1077,  ..., -3.0354, -3.3761, -2.9215]]],\n",
       "\n",
       "\n",
       "        [[[ 7.1472,  7.3238,  7.0774,  ...,  7.2987,  7.3840,  7.7243],\n",
       "          [ 7.1717,  7.5355,  7.1676,  ...,  7.4424,  7.2237,  7.6121],\n",
       "          [ 7.3370,  7.4293,  7.0482,  ...,  6.9459,  7.0843,  7.2371],\n",
       "          ...,\n",
       "          [ 7.3653,  7.4348,  7.3537,  ...,  7.4401,  7.5045,  7.8964],\n",
       "          [ 7.5106,  7.8378,  7.6385,  ...,  7.5543,  7.5076,  7.6977],\n",
       "          [ 7.3677,  7.8862,  7.3681,  ...,  7.7126,  7.4625,  7.8719]],\n",
       "\n",
       "         [[-3.4177, -2.6200, -3.2181,  ..., -3.0135, -3.4967, -2.7110],\n",
       "          [-2.9757, -2.8925, -2.7015,  ..., -2.9632, -2.9253, -2.9649],\n",
       "          [-3.1505, -2.7752, -2.9369,  ..., -3.1478, -3.0483, -2.7119],\n",
       "          ...,\n",
       "          [-2.9480, -2.9281, -2.5930,  ..., -3.1023, -2.8389, -3.0974],\n",
       "          [-3.3928, -2.6496, -3.1452,  ..., -2.9345, -3.3148, -2.6201],\n",
       "          [-2.9869, -2.8857, -2.8586,  ..., -3.0358, -2.9938, -3.0217]],\n",
       "\n",
       "         [[-2.5233, -2.9995, -2.6177,  ..., -2.9646, -2.4662, -3.0527],\n",
       "          [-3.4783, -3.3548, -3.4036,  ..., -3.4854, -3.4466, -3.4469],\n",
       "          [-2.9312, -3.2239, -3.1012,  ..., -3.1507, -2.7481, -3.1063],\n",
       "          ...,\n",
       "          [-3.5031, -3.2670, -3.4367,  ..., -3.3611, -3.5402, -3.4799],\n",
       "          [-2.5593, -3.1748, -2.8204,  ..., -3.1610, -2.6685, -3.1678],\n",
       "          [-3.4709, -3.4840, -3.4581,  ..., -3.4370, -3.5230, -3.4160]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.7411, -2.7745, -2.8775,  ..., -2.9859, -2.7938, -2.7827],\n",
       "          [-2.9708, -3.0972, -2.9411,  ..., -3.3173, -2.7674, -3.0605],\n",
       "          [-2.8927, -3.1199, -2.9384,  ..., -3.2964, -2.7801, -2.8990],\n",
       "          ...,\n",
       "          [-3.2508, -3.2989, -3.1748,  ..., -3.5980, -3.0847, -3.4445],\n",
       "          [-2.7374, -2.8823, -2.8233,  ..., -3.0348, -2.7697, -2.8310],\n",
       "          [-2.8606, -3.2711, -3.0920,  ..., -3.4923, -3.0253, -3.2512]],\n",
       "\n",
       "         [[-3.1301, -2.9653, -2.9885,  ..., -3.0954, -3.1224, -3.1405],\n",
       "          [-3.6497, -3.1530, -3.7314,  ..., -3.1488, -3.7289, -3.2116],\n",
       "          [-3.2363, -2.9874, -3.0874,  ..., -3.0328, -3.1105, -3.0812],\n",
       "          ...,\n",
       "          [-3.4877, -3.2264, -3.5004,  ..., -3.1435, -3.7231, -3.2691],\n",
       "          [-3.2918, -3.1223, -3.2558,  ..., -3.2095, -3.2725, -3.1562],\n",
       "          [-3.5351, -3.2921, -3.4935,  ..., -3.2973, -3.5906, -3.3174]],\n",
       "\n",
       "         [[-2.8738, -3.2176, -3.0456,  ..., -2.9927, -3.1651, -3.2869],\n",
       "          [-3.4968, -2.9221, -3.3473,  ..., -3.0162, -3.5417, -2.9323],\n",
       "          [-2.6955, -3.2269, -2.6662,  ..., -2.8056, -2.8978, -3.2021],\n",
       "          ...,\n",
       "          [-3.1280, -2.9352, -2.9236,  ..., -3.1385, -3.3579, -3.0237],\n",
       "          [-2.9751, -3.4618, -2.8734,  ..., -3.1794, -2.9612, -3.4226],\n",
       "          [-3.4088, -3.0537, -3.0917,  ..., -3.0517, -3.3827, -2.9424]]]],\n",
       "       device='cuda:3', grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet_ssl_model(torch.randn(64, 3, 128, 128).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4346e+08, device='cuda:3', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a[(a<1e8)]\n",
    "b = b[(b>-1e8)]\n",
    "torch.sum(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 21, 128, 128]), torch.Size([81]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape, b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "My acv",
   "language": "python",
   "name": "acv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
