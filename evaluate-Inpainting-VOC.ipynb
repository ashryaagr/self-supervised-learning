{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-pm9GzOk9tMf",
    "outputId": "8431e4e6-8ec3-41be-9c64-d582f8dade4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/My Drive/projects/cse-252-d\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# %cd /content/drive/My Drive/projects/cse-252-d/\n",
    "\n",
    "# # Set the path to your data directory on Google Drive\n",
    "# data_dir = '/dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thtfI8d0ad4q"
   },
   "source": [
    "# Download and unzip the PascalVoc dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "ivkItYJooBYF",
    "outputId": "c787603f-3fcc-4c1b-ae42-c9dad86c22df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/VOCtrainval_06-Nov-2007.tar\n",
      "Extracting ./data/VOCtrainval_06-Nov-2007.tar to ./data\n",
      "Using downloaded and verified file: ./data/VOCtrainval_06-Nov-2007.tar\n",
      "Extracting ./data/VOCtrainval_06-Nov-2007.tar to ./data\n",
      "Using downloaded and verified file: ./data/VOCtest_06-Nov-2007.tar\n",
      "Extracting ./data/VOCtest_06-Nov-2007.tar to ./data\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "train_dataset =torchvision.datasets.VOCSegmentation(root='./data',year='2007',download=True,image_set='train')\n",
    "val_dataset = torchvision.datasets.VOCSegmentation(root='./data',year='2007',download=True,image_set='val')\n",
    "test_dataset = torchvision.datasets.VOCSegmentation(root='./data',year='2007',download=True,image_set='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "W7O2K34Opfen"
   },
   "outputs": [],
   "source": [
    "!cd data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sBPY-IXXn1g4",
    "outputId": "f0c2ed64-22b3-40f7-97a1-e55fa6a2aa13"
   },
   "outputs": [],
   "source": [
    "# !wget http://pjreddie.com/media/files/VOCtest_06-Nov-2007.tar\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "y-ooO_RNn1jb"
   },
   "outputs": [],
   "source": [
    "# !tar -xvf VOCtest_06-Nov-2007.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uKjin4TeaiYN"
   },
   "source": [
    "# Data Preparation\n",
    "\n",
    "run the code below to get thre dataloader objects, namely: train_loader, val_loader and test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q1UV1V2XqkWf",
    "outputId": "04a90050-9610-4011-d28a-6adda25524e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils import data\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "\n",
    "num_classes = 21\n",
    "ignore_label = 255\n",
    "root = './data'\n",
    "\n",
    "'''\n",
    "color map\n",
    "0=background, 1=aeroplane, 2=bicycle, 3=bird, 4=boat, 5=bottle # 6=bus, 7=car, 8=cat, 9=chair, 10=cow, 11=diningtable,\n",
    "12=dog, 13=horse, 14=motorbike, 15=person # 16=potted plant, 17=sheep, 18=sofa, 19=train, 20=tv/monitor\n",
    "'''\n",
    "\n",
    "\n",
    "#Feel free to convert this palette to a map\n",
    "palette = [0, 0, 0, 128, 0, 0, 0, 128, 0, 128, 128, 0, 0, 0, 128, 128, 0, 128, 0, 128, 128,\n",
    "           128, 128, 128, 64, 0, 0, 192, 0, 0, 64, 128, 0, 192, 128, 0, 64, 0, 128, 192, 0, 128,\n",
    "           64, 128, 128, 192, 128, 128, 0, 64, 0, 128, 64, 0, 0, 192, 0, 128, 192, 0, 0, 64, 128]  #3 values- R,G,B for every class. First 3 values for class 0, next 3 for\n",
    "#class 1 and so on......\n",
    "\n",
    "'''\n",
    "Depending on the mode, train or val or test, the function reads the train.txt, val.txt and test.txt files and returns a list of tuples of the form\n",
    "(image_path, mask_path) for each image in the dataset, where image_path is the path to the image and mask_path is the path to the mask for that image. \n",
    "'''\n",
    "def make_dataset(mode):\n",
    "    assert mode in ['train', 'val', 'test']\n",
    "    items = []\n",
    "    if mode == 'train':\n",
    "        img_path = os.path.join(root, 'VOCdevkit', 'VOC2007', 'JPEGImages')\n",
    "        mask_path = os.path.join(root, 'VOCdevkit', 'VOC2007', 'SegmentationClass')\n",
    "        data_list = [l.strip('\\n') for l in open(os.path.join(\n",
    "            root, 'VOCdevkit', 'VOC2007', 'ImageSets', 'Segmentation', 'train.txt')).readlines()]\n",
    "        for it in data_list:\n",
    "            item = (os.path.join(img_path, it + '.jpg'), os.path.join(mask_path, it + '.png'))\n",
    "            items.append(item)\n",
    "    elif mode == 'val':\n",
    "        img_path = os.path.join(root, 'VOCdevkit', 'VOC2007', 'JPEGImages')\n",
    "        mask_path = os.path.join(root, 'VOCdevkit', 'VOC2007', 'SegmentationClass')\n",
    "        data_list = [l.strip('\\n') for l in open(os.path.join(\n",
    "            root, 'VOCdevkit', 'VOC2007', 'ImageSets', 'Segmentation', 'val.txt')).readlines()]\n",
    "        for it in data_list:\n",
    "            item = (os.path.join(img_path, it + '.jpg'), os.path.join(mask_path, it + '.png'))\n",
    "            items.append(item)\n",
    "    else:\n",
    "        img_path = os.path.join(root, 'VOCdevkit', 'VOC2007', 'JPEGImages')\n",
    "        mask_path = os.path.join(root, 'VOCdevkit', 'VOC2007', 'SegmentationClass')\n",
    "        data_list = [l.strip('\\n') for l in open(os.path.join(\n",
    "            root, 'VOCdevkit', 'VOC2007', 'ImageSets', 'Segmentation', 'test.txt')).readlines()]\n",
    "        for it in data_list:\n",
    "            item = (os.path.join(img_path, it + '.jpg'), os.path.join(mask_path, it + '.png'))\n",
    "            items.append(item)\n",
    "    return items\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "The class VOC is a subclass of the class torch.utils.data.Dataset. It overrides the __len__ and __getitem__ methods.\n",
    "The __len__ method returns the length of the dataset, i.e. the number of images in the dataset.\n",
    "The __getitem__ method returns the image and the mask for the given index.\n",
    "'''\n",
    "\n",
    "class VOC(data.Dataset):\n",
    "    def __init__(self, mode, transform=None, target_transform=None, common_transform=None):\n",
    "        self.imgs = make_dataset(mode)\n",
    "        if len(self.imgs) == 0:\n",
    "            raise RuntimeError('Found 0 images, please check the data set')\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.common_transform = common_transform\n",
    "        self.width = 224\n",
    "        self.height = 224\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        img_path, mask_path = self.imgs[index]\n",
    "        img = Image.open(img_path).convert('RGB').resize((self.width, self.height))\n",
    "        mask = Image.open(mask_path).resize((self.width, self.height))\n",
    "\n",
    "        if self.common_transform is not None:\n",
    "            img, mask = self.common_transform((img,mask)) \n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            mask = self.target_transform(mask)\n",
    "\n",
    "        mask[mask==ignore_label]=0\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    voc = VOC(\"train\")\n",
    "\n",
    "    print(len(voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "F9PiYKnVqZyn"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "class MaskToTensor(object):\n",
    "    def __call__(self, img):\n",
    "        return torch.from_numpy(np.array(img, dtype=np.int32)).long()\n",
    "\n",
    "\n",
    "mean_std = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "# common_transform = transforms.Compose([\n",
    "#     # voc.MirrorFlip(0.5),\n",
    "#     # voc.Rotate(10),\n",
    "#     # voc.CenterCrop(180)\n",
    "# ])\n",
    "\n",
    "input_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(*mean_std)\n",
    "])\n",
    "\n",
    "target_transform = transforms.Compose([\n",
    "    MaskToTensor()\n",
    "])\n",
    "\n",
    "# augmented_train_dataset =voc.VOC('train', transform=input_transform, target_transform=target_transform, common_transform=common_transform)\n",
    "# augmented_val_dataset = voc.VOC('val', transform=input_transform, target_transform=target_transform, common_transform=common_transform)\n",
    "# augmented_test_dataset = voc.VOC('test', transform=input_transform, target_transform=target_transform, common_transform=common_transform)\n",
    "\n",
    "original_train_dataset =VOC('train', transform=input_transform, target_transform=target_transform)\n",
    "original_val_dataset = VOC('val', transform=input_transform, target_transform=target_transform)\n",
    "original_test_dataset = VOC('test', transform=input_transform, target_transform=target_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sRm6BoBqxdzw",
    "outputId": "f68df7c5-b344-4b7e-a047-5f3cc46f4756"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "NUM_WORKERS = 4\n",
    "PREFETCH_FACTOR = 2 # improves data transfer speed between GPU and CPU and reduces GPU wait time\n",
    "train_loader = DataLoader(dataset=original_train_dataset, batch_size= 16, shuffle=True, num_workers=NUM_WORKERS, prefetch_factor=PREFETCH_FACTOR, pin_memory=True)\n",
    "val_loader = DataLoader(dataset=original_val_dataset, batch_size= 16, shuffle=False, num_workers=NUM_WORKERS, prefetch_factor=PREFETCH_FACTOR, pin_memory=True)\n",
    "test_loader = DataLoader(dataset=original_test_dataset, batch_size= 16, shuffle=False, num_workers=NUM_WORKERS, prefetch_factor=PREFETCH_FACTOR, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5AiZtwNTbLBE"
   },
   "source": [
    "# end of data section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "IXJsPn8RynHU",
    "outputId": "1e28c02e-26a5-4d25-a0fa-5a23803b541a"
   },
   "outputs": [],
   "source": [
    "# for images, masks in train_loader:\n",
    "#   image = images.to(device)\n",
    "#   mask = masks.to(device)\n",
    "#   break\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# # Assuming data is a tuple (image, label)\n",
    "# image = images[0]\n",
    "# # image = masks[0]\n",
    "# # Convert the tensor image to NumPy array and transpose the dimensions\n",
    "# image = image.numpy().transpose((1, 2, 0))\n",
    "\n",
    "# # Denormalize the image (if it was normalized during transformation)\n",
    "# mean = (0.5, 0.5, 0.5)  # Mean used for normalization\n",
    "# std = (0.5, 0.5, 0.5)  # Standard deviation used for normalization\n",
    "# image = image * std + mean\n",
    "\n",
    "# # Clip the pixel values to [0, 1] range in case of any numerical instability\n",
    "# image = np.clip(image, 0, 1)\n",
    "\n",
    "# # Plot the image\n",
    "# plt.imshow(image)\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCiDan5JzXVS"
   },
   "source": [
    "# utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Y4wq8Kp_UtHf"
   },
   "outputs": [],
   "source": [
    "def iou(pred, target, n_classes = 21):\n",
    "    target[target==255] = 0\n",
    "\n",
    "    ious = []\n",
    "\n",
    "    for cls in range(n_classes):\n",
    "        intersection = torch.sum((pred == cls) & (target == cls)).item()\n",
    "        union = torch.sum(pred == cls) + torch.sum(target == cls) - intersection\n",
    "        union = union.item()\n",
    "        if union!=0:\n",
    "            ious.append(intersection/union)\n",
    "\n",
    "    ious = np.array(ious)\n",
    "    return ious\n",
    "\n",
    "'''\n",
    "returns pixel accuracy for the batch\n",
    "'''\n",
    "def pixel_acc(pred, target):\n",
    "    target[target==255] = 0\n",
    "    \n",
    "    correct = torch.sum(pred==target).item()\n",
    "    total_predictions = target.shape[0]*target.shape[1]*target.shape[2]\n",
    "    return correct/total_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "nIMKgG9GRiel"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.nn.functional as F\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model=None):\n",
    "\n",
    "    model_ = model \n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    \n",
    "    best_iou_score = 0.0\n",
    "\n",
    "    trainEpochLoss = []\n",
    "    trainEpochAccuracy = []\n",
    "    trainEpochIOU = []\n",
    "    valEpochLoss = []\n",
    "    valEpochAccuracy = []\n",
    "    valEpochIOU = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "        train_loss = []\n",
    "        train_acc = []\n",
    "        train_iou = []\n",
    "\n",
    "        ts = time.time()\n",
    "        for iter, (inputs, labels) in enumerate(train_loader):\n",
    "            #   reset optimizer gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            # both inputs and labels have to reside in the same device as the model's\n",
    "            inputs =  inputs.to(device)#  transfer the input to the same device as the model's\n",
    "            labels =   labels.to(device)#  transfer the labels to the same device as the model's\n",
    "\n",
    "            trainOutputs =  model_.forward(inputs) #   Compute outputs. we will not need to transfer the output, it will be automatically in the same device as the model's!\n",
    "            trainOutputs = F.softmax(trainOutputs)\n",
    "            loss =  criterion(trainOutputs,labels)  #  calculate loss\n",
    "            loss.backward()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # To compute accuracy and IOU\n",
    "                # outputs = F.log_softmax(model_(inputs), dim=1)\n",
    "                _, pred = torch.max(trainOutputs, dim=1)\n",
    "                \n",
    "                train_iou += [np.mean(iou(pred, labels))]\n",
    "                train_acc += [pixel_acc(pred, labels)]\n",
    "                train_loss.append(loss.item())\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if iter % 10 == 0:\n",
    "                print(f\"==> epoch{epoch}, iter{iter}, Train set=> loss: {train_loss[-1]}, IOU: {train_iou[-1]}, Acc: {train_acc[-1]}\")\n",
    "\n",
    "        # print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n",
    "\n",
    "        print(\"Finish epoch {}, time elapsed {}\".format(epoch, time.time() - ts))\n",
    "\n",
    "        val_loss, val_iou, val_acc = val(epoch,model_)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ##### Plotting values\n",
    "        trainEpochLoss.append(np.mean(np.asarray(train_loss)))\n",
    "        trainEpochIOU.append(np.mean(np.asarray(train_iou)))\n",
    "        trainEpochAccuracy.append(np.mean(train_acc))\n",
    "        valEpochLoss.append(val_loss)\n",
    "        valEpochIOU.append(val_iou)\n",
    "        valEpochAccuracy.append(val_acc)\n",
    "\n",
    "    # plots(trainEpochLoss, trainEpochAccuracy, trainEpochIOU, valEpochLoss, valEpochAccuracy, valEpochIOU, best_iter, saveLocation=saveLocation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "UTQQ4B7hU3cd"
   },
   "outputs": [],
   "source": [
    "def val(epoch, model=None):\n",
    "    model_ = model\n",
    "    model_.eval() # Put in eval mode (disables batchnorm/dropout) !\n",
    "    \n",
    "    losses = []\n",
    "    mean_iou_scores = []\n",
    "    accuracy = []\n",
    "\n",
    "    with torch.no_grad(): # we don't need to calculate the gradient in the validation/testing\n",
    "        num_iter = 0\n",
    "        for iter, (inputs, labels) in enumerate(val_loader):\n",
    "            \n",
    "            # both inputs and labels have to reside in the same device as the model's\n",
    "            inputs =  inputs.to(device)#  transfer the input to the same device as the model's\n",
    "            labels =   labels.to(device)#  transfer the labels to the same device as the model's\n",
    "\n",
    "\n",
    "            outputs = F.softmax(model_(inputs), dim=1)\n",
    "#             valoutputs = model_(inputs)\n",
    "            valloss = criterion(outputs, labels)\n",
    "            \n",
    "            num_iter += 1\n",
    "            _, pred = torch.max(outputs, dim=1)\n",
    "            mean_iou_scores += [np.mean(iou(pred, labels))]\n",
    "            accuracy += [pixel_acc(pred, labels)]\n",
    "            losses += [valloss.item()]\n",
    "\n",
    "    # print(mean_iou_scores, accuracy)\n",
    "    print(f\"=========> Loss at epoch {epoch} is {np.mean(losses)}\")\n",
    "    print(f\"=========> IoU at epoch {epoch} is {np.mean(mean_iou_scores)}\")\n",
    "    print(f\"=========> Pixel acc at epoch {epoch} is {np.mean(accuracy)}\")\n",
    "\n",
    "    model_.train() #TURNING THE TRAIN MODE BACK ON TO ENABLE BATCHNORM/DROPOUT!!\n",
    "\n",
    "    return np.mean(losses), np.mean(mean_iou_scores), np.mean(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "loHUcdGAb_Xy"
   },
   "source": [
    "# SSL models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "num_scales = 3\n",
    "scale_factor = 2\n",
    "\n",
    "# Define the network architecture with feature pyramid\n",
    "class CompletionModel(nn.Module):\n",
    "    def __init__(self, num_scales=3):\n",
    "        super(CompletionModel, self).__init__()\n",
    "        self.num_scales = num_scales\n",
    "\n",
    "        # Define layers for each scale in the feature pyramid\n",
    "        self.encoders = nn.ModuleList()\n",
    "        self.decoders = nn.ModuleList()\n",
    "        self.channel_reducers = nn.ModuleList()  # 1x1 convolution layers to reduce channels\n",
    "#         self.downsamplers = nn.ModuleList()\n",
    "#         self.upsamplers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_scales):\n",
    "            encoder = nn.Sequential(\n",
    "                nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            self.encoders.append(encoder)\n",
    "\n",
    "            decoder = nn.Sequential(\n",
    "                nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose2d(64, 3, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            self.decoders.append(decoder)\n",
    "            \n",
    "            channel_reducer = nn.Conv2d(3, 512, kernel_size=1)  # 1x1 convolution layer to reduce channels\n",
    "            self.channel_reducers.append(channel_reducer)\n",
    "            \n",
    "        self.upsampler = nn.Upsample(scale_factor=scale_factor, mode='bilinear', align_corners=False)\n",
    "        self.downsampler = nn.Upsample(scale_factor=1/scale_factor, mode='bilinear', align_corners=False)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize list to store features from each scale\n",
    "        features = []\n",
    "        #print(f\"x.shape: {x.shape}\")\n",
    "\n",
    "        # Forward pass through each scale in the feature pyramid\n",
    "        for i in range(self.num_scales):\n",
    "            encoder_output = self.encoders[i](x)\n",
    "            features.append(encoder_output)\n",
    "            #print(f\"Feature size at {i}th scale: {features[-1].shape}\")\n",
    "            x = self.downsampler(x)\n",
    "\n",
    "        # Decode the concatenated features\n",
    "        shape_feats = list(features[-1].shape)\n",
    "        shape_feats[1] = 3\n",
    "        prev_output = None#torch.zeros(shape_feats).to(device)\n",
    "        \n",
    "        for i in range(self.num_scales):\n",
    "            decoder_output = self.decoders[i](features[self.num_scales - i - 1])\n",
    "            #print(f\"decoder output shape: {decoder_output.shape}\")\n",
    "            if prev_output is None:\n",
    "                prev_output = self.upsampler(decoder_output)\n",
    "                #print(f\"prev output is none: prev_output.shape : {prev_output.shape}\")\n",
    "            else:\n",
    "                #prev_output = self.channel_reducers[i](prev_output)\n",
    "                #print(f\"prev_output.shape : {prev_output.shape}\")\n",
    "                prev_output = self.upsampler(prev_output+decoder_output)\n",
    "        \n",
    "        #print(decoder_output.shape)\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self supervised training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "image_size = 28\n",
    "\n",
    "# Gaussian Pyramid Constants\n",
    "num_scales = 3\n",
    "scale_factor = 2\n",
    "batch_size = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssl_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(*mean_std)\n",
    "])\n",
    "\n",
    "target_transform = transforms.Compose([\n",
    "    MaskToTensor()\n",
    "])\n",
    "\n",
    "ssl_dataset_train = VOC('train', transform=ssl_transform, target_transform=target_transform)\n",
    "ssl_dataset_val = VOC('val', transform=ssl_transform, target_transform=target_transform)\n",
    "ssl_dataset_test = VOC('test', transform=ssl_transform, target_transform=target_transform)\n",
    "\n",
    "combined_dataset = torch.utils.data.ConcatDataset([ssl_dataset_train, ssl_dataset_val, ssl_dataset_test])\n",
    "ssl_dataloader = DataLoader(dataset=combined_dataset, batch_size= 16, shuffle=True, num_workers=NUM_WORKERS, prefetch_factor=PREFETCH_FACTOR, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.2193\n",
      "Epoch [2/10], Loss: 1.1501\n",
      "Epoch [3/10], Loss: 1.1275\n",
      "Epoch [4/10], Loss: 1.1312\n",
      "Epoch [5/10], Loss: 1.0948\n",
      "Epoch [6/10], Loss: 1.0943\n",
      "Epoch [7/10], Loss: 1.0910\n",
      "Epoch [8/10], Loss: 1.0871\n",
      "Epoch [9/10], Loss: 1.0627\n",
      "Epoch [10/10], Loss: 1.0850\n"
     ]
    }
   ],
   "source": [
    "model = CompletionModel().to(device)\n",
    "# model.load_state_dict(torch.load('ssl_model.pth'))\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, _ in ssl_dataloader:\n",
    "        images = images.to(device)#.unsqueeze(0)\n",
    "        #print(images.shape)\n",
    "        \n",
    "        occluded_image = images.clone()\n",
    "        _,_, h, w = occluded_image.shape\n",
    "        x = torch.randint(0, w // 2, (1,))\n",
    "        y = torch.randint(0, h // 2, (1,))\n",
    "        occluded_image[:, :, y:y + h // 2, x:x + w // 2] = 0\n",
    "        # Forward pass and loss calculation\n",
    "        completion_images = model(occluded_image.to(device))\n",
    "        loss = criterion(completion_images[:, :, y:y + h // 2, x:x + w // 2], images[:, :, y:y + h // 2, x:x + w // 2])\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Compute the average loss for the epoch\n",
    "    average_loss = running_loss / len(ssl_dataloader)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}\")\n",
    "\n",
    "# Generate completions for test images\n",
    "model.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRnApfp4E5ok"
   },
   "source": [
    "## SSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "xkWyigy-PQhI"
   },
   "outputs": [],
   "source": [
    "class Backbone(nn.Module):\n",
    "    def __init__(self, out_dim):\n",
    "        super(Backbone, self).__init__()\n",
    "        \n",
    "        # Define the layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Define the fully connected (linear) layer\n",
    "        self.fc = nn.Linear(in_features=1568, out_features=out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Perform forward pass\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "_1Zwqss-PIJl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class UNet_ssl(nn.Module):\n",
    "    def __init__(self, n_class=21, n_dim=512):\n",
    "        super(UNet_ssl, self).__init__()\n",
    "\n",
    "        # Encoder (Based on the provided SSL architecture)\n",
    "        #self.encoder = Backbone(out_dim=n_dim)\n",
    "        self.encoder = nn.Sequential(\n",
    "                nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
    "                nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder4 = self.expanding_block(n_dim, 256)\n",
    "        self.decoder3 = self.expanding_block(256, 128)\n",
    "        self.decoder2 = self.expanding_block(128, 64)\n",
    "        self.decoder1 = self.expanding_block(64, 32)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output = nn.Conv2d(32, n_class, kernel_size=1)\n",
    "        \n",
    "    def expanding_block(self, in_channels, out_channels):\n",
    "        block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(out_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "        )\n",
    "        return block\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder (SSL model)\n",
    "        features = self.encoder(x)\n",
    "        \n",
    "        # Decoder\n",
    "        decode4 = self.decoder4(features)\n",
    "        decode3 = self.decoder3(decode4)\n",
    "        decode2 = self.decoder2(decode3)\n",
    "        decode1 = self.decoder1(decode2)\n",
    "        \n",
    "        # Output\n",
    "        output = self.output(decode1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with SSL Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "ZQONHMeGPIPO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2906552/3566168316.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  trainOutputs = F.softmax(trainOutputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> epoch0, iter0, Train set=> loss: 3.0392708778381348, IOU: 0.04204779596209764, Acc: 0.670478665098852\n",
      "==> epoch0, iter10, Train set=> loss: 2.375284194946289, IOU: 0.05342035321383017, Acc: 0.7478849449936225\n",
      "Finish epoch 0, time elapsed 8.925754308700562\n",
      "=========> Loss at epoch 0 is 2.3723459243774414\n",
      "=========> IoU at epoch 0 is 0.05564781345682834\n",
      "=========> Pixel acc at epoch 0 is 0.7508232094456085\n",
      "==> epoch1, iter0, Train set=> loss: 2.330216646194458, IOU: 0.05663946835709731, Acc: 0.7929525569993623\n",
      "==> epoch1, iter10, Train set=> loss: 2.299797296524048, IOU: 0.048433631265006, Acc: 0.823371731505102\n",
      "Finish epoch 1, time elapsed 8.598904371261597\n",
      "=========> Loss at epoch 1 is 2.3723459243774414\n",
      "=========> IoU at epoch 1 is 0.05564781345682834\n",
      "=========> Pixel acc at epoch 1 is 0.7508232094456085\n",
      "==> epoch2, iter0, Train set=> loss: 2.3831615447998047, IOU: 0.049333844866071425, Acc: 0.7400076729910714\n",
      "==> epoch2, iter10, Train set=> loss: 2.4431939125061035, IOU: 0.04856965979751276, Acc: 0.6799752371651786\n",
      "Finish epoch 2, time elapsed 8.580034732818604\n",
      "=========> Loss at epoch 2 is 2.3723459243774414\n",
      "=========> IoU at epoch 2 is 0.05564781345682834\n",
      "=========> Pixel acc at epoch 2 is 0.7508232094456085\n",
      "==> epoch3, iter0, Train set=> loss: 2.4407637119293213, IOU: 0.04549369552508504, Acc: 0.6824054328762755\n",
      "==> epoch3, iter10, Train set=> loss: 2.3736722469329834, IOU: 0.05353548366891399, Acc: 0.7494967713647959\n",
      "Finish epoch 3, time elapsed 8.602560043334961\n",
      "=========> Loss at epoch 3 is 2.3723459243774414\n",
      "=========> IoU at epoch 3 is 0.05564781345682834\n",
      "=========> Pixel acc at epoch 3 is 0.7508232094456085\n",
      "==> epoch4, iter0, Train set=> loss: 2.386683940887451, IOU: 0.04603031703404018, Acc: 0.7364850725446429\n",
      "==> epoch4, iter10, Train set=> loss: 2.4549598693847656, IOU: 0.041763072111168684, Acc: 0.6682091537786989\n",
      "Finish epoch 4, time elapsed 8.57461929321289\n",
      "=========> Loss at epoch 4 is 2.3723459243774414\n",
      "=========> IoU at epoch 4 is 0.05564781345682834\n",
      "=========> Pixel acc at epoch 4 is 0.7508232094456085\n",
      "==> epoch5, iter0, Train set=> loss: 2.457119941711426, IOU: 0.047574946901193516, Acc: 0.6660492566167092\n",
      "==> epoch5, iter10, Train set=> loss: 2.368649959564209, IOU: 0.05030127285289116, Acc: 0.7545190927933674\n",
      "Finish epoch 5, time elapsed 8.580931425094604\n",
      "=========> Loss at epoch 5 is 2.3723459243774414\n",
      "=========> IoU at epoch 5 is 0.05564781345682834\n",
      "=========> Pixel acc at epoch 5 is 0.7508232094456085\n",
      "==> epoch6, iter0, Train set=> loss: 2.3695743083953857, IOU: 0.050239656409438775, Acc: 0.7535948461415817\n",
      "==> epoch6, iter10, Train set=> loss: 2.4068281650543213, IOU: 0.0421377046912515, Acc: 0.7163409797512755\n",
      "Finish epoch 6, time elapsed 8.565086841583252\n",
      "=========> Loss at epoch 6 is 2.3723459243774414\n",
      "=========> IoU at epoch 6 is 0.05564781345682834\n",
      "=========> Pixel acc at epoch 6 is 0.7508232094456085\n",
      "==> epoch7, iter0, Train set=> loss: 2.3614752292633057, IOU: 0.05440670269223761, Acc: 0.7616938376913265\n",
      "==> epoch7, iter10, Train set=> loss: 2.412414789199829, IOU: 0.044422149658203125, Acc: 0.71075439453125\n",
      "Finish epoch 7, time elapsed 8.447433710098267\n",
      "=========> Loss at epoch 7 is 2.3723459243774414\n",
      "=========> IoU at epoch 7 is 0.05564781345682834\n",
      "=========> Pixel acc at epoch 7 is 0.7508232094456085\n",
      "==> epoch8, iter0, Train set=> loss: 2.3811872005462646, IOU: 0.043645998438437875, Acc: 0.7419819734534439\n",
      "==> epoch8, iter10, Train set=> loss: 2.3597986698150635, IOU: 0.0587208027742347, Acc: 0.7633704360650511\n",
      "Finish epoch 8, time elapsed 8.593524932861328\n",
      "=========> Loss at epoch 8 is 2.3723459243774414\n",
      "=========> IoU at epoch 8 is 0.05564781345682834\n",
      "=========> Pixel acc at epoch 8 is 0.7508232094456085\n",
      "==> epoch9, iter0, Train set=> loss: 2.4063198566436768, IOU: 0.04778994605654762, Acc: 0.7168491908482143\n",
      "==> epoch9, iter10, Train set=> loss: 2.4077398777008057, IOU: 0.055033014557986655, Acc: 0.7154291892538265\n",
      "Finish epoch 9, time elapsed 8.56226897239685\n",
      "=========> Loss at epoch 9 is 2.3723459243774414\n",
      "=========> IoU at epoch 9 is 0.05564781345682834\n",
      "=========> Pixel acc at epoch 9 is 0.7508232094456085\n"
     ]
    }
   ],
   "source": [
    "# Create the U-Net model\n",
    "\n",
    "unet_ssl_model = UNet_ssl(n_class=21, n_dim=512)  # Assuming 21 classes for segmentation\n",
    "\n",
    "# Load the SSL model weights\n",
    "# ssl_model_weights = torch.load('my_ssl.pth', map_location=torch.device(device))\n",
    "# unet_ssl_model.encoder.load_state_dict(ssl_model_weights)\n",
    "unet_ssl_model.encoder = model.encoders[0]\n",
    "\n",
    "# Move the U-Net model to the desired device (e.g., GPU)\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "unet_ssl_model = unet_ssl_model.to(device)\n",
    "\n",
    "# # Print the U-Net model architecture\n",
    "# print(unet_ssl_model)\n",
    "\n",
    "epochs = 10\n",
    "from torch import optim\n",
    "optimizer = optim.Adam(unet_ssl_model.parameters(), lr=0.005)#  choose an optimizer\n",
    "train(unet_ssl_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "id": "GbCVDRxGPIRF",
    "outputId": "bcff3e81-c437-45f8-a6fa-5ca8a16ab5b2"
   },
   "source": [
    "## Training without SSL Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "byUEBl3EQehW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2906552/3566168316.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  trainOutputs = F.softmax(trainOutputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> epoch0, iter0, Train set=> loss: 2.4303665161132812, IOU: 0.04618683899340987, Acc: 0.692802584901148\n",
      "==> epoch0, iter10, Train set=> loss: 2.3992648124694824, IOU: 0.0452440222915338, Acc: 0.7239043566645408\n",
      "Finish epoch 0, time elapsed 8.868607997894287\n",
      "=========> Loss at epoch 0 is 2.3723459243774414\n",
      "=========> IoU at epoch 0 is 0.05564781345682834\n",
      "=========> Pixel acc at epoch 0 is 0.7508232094456085\n",
      "==> epoch1, iter0, Train set=> loss: 2.3283767700195312, IOU: 0.044155129499716554, Acc: 0.794792330994898\n",
      "==> epoch1, iter10, Train set=> loss: 2.41565203666687, IOU: 0.047167802667942176, Acc: 0.7075170400191326\n",
      "Finish epoch 1, time elapsed 8.559503078460693\n",
      "=========> Loss at epoch 1 is 2.3723459243774414\n",
      "=========> IoU at epoch 1 is 0.05564781345682834\n",
      "=========> Pixel acc at epoch 1 is 0.7508232094456085\n",
      "==> epoch2, iter0, Train set=> loss: 2.32694935798645, IOU: 0.053081320418792516, Acc: 0.7962198062818877\n",
      "==> epoch2, iter10, Train set=> loss: 2.3953099250793457, IOU: 0.04549119910415338, Acc: 0.7278591856664541\n",
      "Finish epoch 2, time elapsed 8.678443908691406\n",
      "=========> Loss at epoch 2 is 2.3723459243774414\n",
      "=========> IoU at epoch 2 is 0.05564781345682834\n",
      "=========> Pixel acc at epoch 2 is 0.7508232094456085\n",
      "==> epoch3, iter0, Train set=> loss: 2.4691550731658936, IOU: 0.05945582911786874, Acc: 0.6540141202965561\n",
      "==> epoch3, iter10, Train set=> loss: 2.3300833702087402, IOU: 0.05664898841791181, Acc: 0.7930858378507653\n",
      "Finish epoch 3, time elapsed 8.677056312561035\n",
      "=========> Loss at epoch 3 is 2.3723459243774414\n",
      "=========> IoU at epoch 3 is 0.05564781345682834\n",
      "=========> Pixel acc at epoch 3 is 0.7508232094456085\n",
      "==> epoch4, iter0, Train set=> loss: 2.347381114959717, IOU: 0.0596759981787186, Acc: 0.7757879763233418\n",
      "==> epoch4, iter10, Train set=> loss: 2.397618055343628, IOU: 0.04534694126674107, Acc: 0.7255510602678571\n",
      "Finish epoch 4, time elapsed 8.546910762786865\n",
      "=========> Loss at epoch 4 is 2.3723459243774414\n",
      "=========> IoU at epoch 4 is 0.05564781345682834\n",
      "=========> Pixel acc at epoch 4 is 0.7508232094456085\n",
      "==> epoch5, iter0, Train set=> loss: 2.4125404357910156, IOU: 0.05921904894770408, Acc: 0.7106285873724489\n",
      "==> epoch5, iter10, Train set=> loss: 2.3696813583374023, IOU: 0.04432280724789916, Acc: 0.7534877232142857\n",
      "Finish epoch 5, time elapsed 8.872746706008911\n",
      "=========> Loss at epoch 5 is 2.3723459243774414\n",
      "=========> IoU at epoch 5 is 0.05564781345682834\n",
      "=========> Pixel acc at epoch 5 is 0.7508232094456085\n",
      "==> epoch6, iter0, Train set=> loss: 2.4001517295837402, IOU: 0.04016763769309807, Acc: 0.7230174784757653\n",
      "==> epoch6, iter10, Train set=> loss: 2.450845718383789, IOU: 0.048023101539723036, Acc: 0.6723234215561225\n",
      "Finish epoch 6, time elapsed 8.513664245605469\n",
      "=========> Loss at epoch 6 is 2.3723459243774414\n",
      "=========> IoU at epoch 6 is 0.05564781345682834\n",
      "=========> Pixel acc at epoch 6 is 0.7508232094456085\n",
      "==> epoch7, iter0, Train set=> loss: 2.446878433227539, IOU: 0.04226816916952328, Acc: 0.6762907067123725\n",
      "==> epoch7, iter10, Train set=> loss: 2.35487961769104, IOU: 0.05487781224375911, Acc: 0.7682893714126275\n",
      "Finish epoch 7, time elapsed 9.156397342681885\n",
      "=========> Loss at epoch 7 is 2.3723459243774414\n",
      "=========> IoU at epoch 7 is 0.05564781345682834\n",
      "=========> Pixel acc at epoch 7 is 0.7508232094456085\n",
      "==> epoch8, iter0, Train set=> loss: 2.383423089981079, IOU: 0.05690354567307692, Acc: 0.73974609375\n",
      "==> epoch8, iter10, Train set=> loss: 2.338068962097168, IOU: 0.05234001315369898, Acc: 0.7851001973054847\n",
      "Finish epoch 8, time elapsed 8.881089687347412\n",
      "=========> Loss at epoch 8 is 2.3723459243774414\n",
      "=========> IoU at epoch 8 is 0.05564781345682834\n",
      "=========> Pixel acc at epoch 8 is 0.7508232094456085\n",
      "==> epoch9, iter0, Train set=> loss: 2.3169944286346436, IOU: 0.050385922801737884, Acc: 0.8061747648278061\n",
      "==> epoch9, iter10, Train set=> loss: 2.3470659255981445, IOU: 0.05174020780187075, Acc: 0.7761031170280612\n",
      "Finish epoch 9, time elapsed 8.590617656707764\n",
      "=========> Loss at epoch 9 is 2.3723459243774414\n",
      "=========> IoU at epoch 9 is 0.05564781345682834\n",
      "=========> Pixel acc at epoch 9 is 0.7508232094456085\n"
     ]
    }
   ],
   "source": [
    "# Create the U-Net model\n",
    "\n",
    "unet_model = UNet_ssl(n_class=21, n_dim=512)  # Assuming 21 classes for segmentation\n",
    "unet_model = unet_ssl_model.to(device)\n",
    "optimizer = optim.Adam(unet_model.parameters(), lr=0.005)#  choose an optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "epochs = 10\n",
    "train(unet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "My acv",
   "language": "python",
   "name": "acv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
