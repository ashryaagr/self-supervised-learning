{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f1a41bc",
   "metadata": {},
   "source": [
    "## Gaussian Pyramid Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91d89097",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2149805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install online_triplet_loss\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from online_triplet_loss.losses import *\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch import linalg\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.cluster import KMeans\n",
    "from copy import deepcopy\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from data import get_triplets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59379b8b",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a6474ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Backbone(nn.Module):\n",
    "    def __init__(self, in_chan, out_dim, num_classes):\n",
    "        super(Backbone, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=in_chan, out_channels=16, kernel_size=3, stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.maxpool5 = nn.MaxPool2d(kernel_size=2, stride=2)        \n",
    "        \n",
    "        self.pooling = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        self.fc = nn.Linear(in_features=256, out_features=out_dim)\n",
    "        self.relu6 = nn.ReLU()\n",
    "        \n",
    "        self.classifier = nn.Linear(out_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.bn1(self.conv1(x)))\n",
    "        x = self.relu2(self.bn2(self.conv2(x)))\n",
    "        x = self.relu3(self.bn3(self.conv3(x)))\n",
    "\n",
    "        x = self.relu4(self.bn4(self.conv4(x)))\n",
    "        x = self.maxpool4(x)\n",
    "        \n",
    "        x = self.relu5(self.bn5(self.conv5(x)))\n",
    "        x = self.maxpool5(x)\n",
    "\n",
    "        x = self.pooling(x)\n",
    "        embed = self.fc(x.squeeze())\n",
    "        \n",
    "        x = self.relu6(embed)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x, embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77ea8917",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PetDataset(Dataset):\n",
    "    def __init__(self, flist, transform, labels):\n",
    "        \n",
    "        self.flist = flist\n",
    "        self.transform = transform\n",
    "        self.labels = np.array(labels).astype(\"int64\")\n",
    "        assert len(flist) == len(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.flist)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.flist[index]\n",
    "\n",
    "        # read in the image, apply the standard transformation\n",
    "        img = self.transform(Image.open(sample))\n",
    "\n",
    "        return img, self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "265f6883",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26baa6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the augmented images\n",
    "src = \"pet/train/output\"\n",
    "flist_full = [os.path.join(src, f) for f in sorted(os.listdir(src))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb0394e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSteps in every epoch:\\n\\n1. Create random labels, create a dataset, dataloader with these labels\\n2. Do a forward & backward pass, using the feature vectors perform k-means clustering\\n3. Use cluster assignments as the labels and redefine the dataset and dataloader\\n4. Go to 2\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Steps in every epoch:\n",
    "\n",
    "1. Create random labels, create a dataset, dataloader with these labels\n",
    "2. Do a forward & backward pass, using the feature vectors perform k-means clustering\n",
    "3. Use cluster assignments as the labels and redefine the dataset and dataloader\n",
    "4. Go to 2\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24cc01df",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 20\n",
    "NUM_CLUSTERS = NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bab20580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster the features\n",
    "def cluster_features(features, num_clusters=5):\n",
    "    \n",
    "    cobj = KMeans(n_clusters=num_clusters)\n",
    "    cobj.fit(features)    \n",
    "    assignments = cobj.labels_\n",
    "    \n",
    "    return assignments \n",
    "\n",
    "\n",
    "def get_sizes(labels):\n",
    "    \n",
    "    clusters = defaultdict(int)\n",
    "    for l in labels:\n",
    "        clusters[l] += 1\n",
    "    \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4227f80b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 499/500 [01:21<00:00,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 3.0030\n",
      "Size of clusters: defaultdict(<class 'int'>, {11: 146, 17: 161, 19: 15, 13: 379, 16: 455, 2: 1877, 1: 48, 5: 202, 4: 104, 6: 1729, 15: 1124, 3: 48, 7: 101, 14: 209, 8: 349, 0: 682, 10: 169, 9: 130, 18: 37, 12: 35})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 499/500 [01:20<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Loss: 2.4623\n",
      "Size of clusters: defaultdict(<class 'int'>, {13: 1065, 5: 765, 8: 171, 6: 1091, 14: 242, 7: 59, 1: 108, 2: 455, 10: 51, 3: 29, 17: 86, 12: 784, 15: 25, 16: 324, 18: 70, 9: 116, 11: 153, 4: 601, 19: 687, 0: 1118})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 499/500 [01:20<00:00,  6.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20], Loss: 2.5926\n",
      "Size of clusters: defaultdict(<class 'int'>, {17: 96, 10: 21, 2: 213, 0: 66, 15: 81, 7: 404, 3: 59, 14: 415, 18: 171, 8: 189, 6: 426, 16: 7, 4: 447, 19: 552, 12: 1329, 1: 1455, 11: 826, 13: 499, 9: 198, 5: 546})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 499/500 [01:20<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20], Loss: 2.6187\n",
      "Size of clusters: defaultdict(<class 'int'>, {4: 295, 14: 572, 6: 741, 16: 737, 18: 409, 7: 148, 12: 834, 19: 304, 5: 12, 2: 676, 11: 632, 0: 898, 8: 487, 17: 212, 10: 70, 1: 154, 9: 365, 3: 177, 13: 127, 15: 150})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 499/500 [01:20<00:00,  6.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20], Loss: 2.7891\n",
      "Size of clusters: defaultdict(<class 'int'>, {15: 124, 1: 73, 8: 68, 7: 130, 9: 162, 13: 31, 6: 167, 18: 406, 4: 360, 12: 93, 10: 813, 2: 1470, 17: 795, 16: 567, 3: 223, 0: 347, 11: 239, 5: 603, 19: 975, 14: 354})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 499/500 [01:20<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20], Loss: 2.6650\n",
      "Size of clusters: defaultdict(<class 'int'>, {11: 49, 6: 74, 1: 78, 18: 92, 5: 34, 19: 87, 17: 40, 10: 109, 2: 239, 3: 21, 13: 67, 8: 257, 14: 282, 12: 191, 16: 673, 4: 732, 7: 267, 9: 906, 0: 2193, 15: 1609})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 499/500 [01:20<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20], Loss: 2.3402\n",
      "Size of clusters: defaultdict(<class 'int'>, {12: 128, 1: 96, 18: 142, 8: 291, 13: 42, 2: 103, 10: 86, 9: 40, 5: 8, 14: 98, 16: 67, 3: 163, 17: 57, 6: 375, 15: 237, 0: 1688, 7: 1058, 19: 1456, 4: 1076, 11: 789})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 499/500 [01:17<00:00,  6.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20], Loss: 2.4066\n",
      "Size of clusters: defaultdict(<class 'int'>, {8: 98, 16: 63, 2: 63, 18: 80, 6: 96, 9: 96, 13: 144, 4: 144, 0: 160, 17: 192, 3: 656, 10: 464, 7: 704, 11: 976, 15: 624, 5: 1248, 19: 448, 14: 624, 1: 384, 12: 736})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 499/500 [01:19<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20], Loss: 2.7264\n",
      "Size of clusters: defaultdict(<class 'int'>, {12: 112, 3: 80, 14: 80, 7: 64, 15: 96, 1: 128, 8: 160, 10: 208, 5: 208, 19: 176, 2: 336, 17: 192, 13: 304, 18: 704, 4: 576, 11: 704, 6: 1296, 16: 912, 0: 656, 9: 1008})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 499/500 [01:19<00:00,  6.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20], Loss: 2.7586\n",
      "Size of clusters: defaultdict(<class 'int'>, {5: 96, 14: 64, 1: 80, 10: 48, 16: 64, 4: 64, 18: 64, 8: 64, 12: 64, 13: 112, 0: 944, 19: 304, 6: 416, 11: 560, 3: 656, 17: 320, 2: 1312, 9: 1872, 7: 512, 15: 384})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 499/500 [01:19<00:00,  6.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20], Loss: 2.5238\n",
      "Size of clusters: defaultdict(<class 'int'>, {0: 96, 9: 256, 16: 688, 4: 944, 18: 416, 12: 944, 2: 272, 15: 320, 6: 384, 17: 112, 11: 128, 3: 192, 8: 208, 13: 128, 19: 288, 1: 624, 5: 880, 14: 592, 7: 224, 10: 304})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 499/500 [01:19<00:00,  6.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20], Loss: 2.9217\n",
      "Size of clusters: defaultdict(<class 'int'>, {11: 320, 8: 400, 1: 694, 7: 141, 4: 99, 14: 18, 13: 50, 2: 130, 18: 214, 17: 105, 6: 431, 10: 851, 12: 277, 19: 251, 3: 134, 5: 2221, 9: 795, 16: 436, 15: 56, 0: 377})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 499/500 [01:20<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20], Loss: 2.5435\n",
      "Size of clusters: defaultdict(<class 'int'>, {9: 143, 15: 7, 10: 53, 7: 32, 1: 119, 6: 289, 19: 179, 13: 169, 11: 202, 5: 56, 2: 130, 16: 274, 0: 575, 17: 751, 4: 412, 18: 188, 12: 1323, 8: 1476, 14: 662, 3: 960})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 499/500 [01:20<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20], Loss: 2.6321\n",
      "Size of clusters: defaultdict(<class 'int'>, {8: 128, 4: 96, 6: 112, 13: 96, 12: 256, 0: 464, 5: 240, 11: 240, 18: 336, 2: 448, 7: 816, 17: 240, 1: 304, 10: 624, 19: 368, 9: 320, 3: 577, 15: 1056, 14: 991, 16: 288})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 499/500 [01:19<00:00,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20], Loss: 2.8572\n",
      "Size of clusters: defaultdict(<class 'int'>, {3: 240, 17: 176, 1: 208, 11: 273, 18: 108, 2: 106, 19: 122, 9: 92, 8: 102, 12: 143, 5: 124, 10: 116, 6: 182, 16: 190, 0: 1530, 7: 1679, 15: 980, 14: 640, 13: 676, 4: 313})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 499/500 [01:19<00:00,  6.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20], Loss: 2.5388\n",
      "Size of clusters: defaultdict(<class 'int'>, {15: 279, 7: 490, 4: 414, 6: 504, 13: 1338, 0: 1730, 11: 1427, 3: 681, 12: 862, 5: 15, 8: 47, 17: 50, 14: 1, 18: 28, 2: 23, 9: 17, 16: 16, 1: 20, 19: 29, 10: 29})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 499/500 [01:20<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20], Loss: 2.2266\n",
      "Size of clusters: defaultdict(<class 'int'>, {4: 122, 12: 82, 17: 65, 2: 188, 6: 44, 18: 9, 9: 245, 1: 88, 8: 25, 7: 1571, 19: 24, 11: 676, 14: 76, 13: 185, 10: 345, 5: 502, 3: 83, 15: 255, 16: 972, 0: 2443})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 499/500 [01:20<00:00,  6.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20], Loss: 2.2622\n",
      "Size of clusters: defaultdict(<class 'int'>, {17: 20, 5: 106, 1: 104, 13: 70, 10: 20, 16: 193, 11: 43, 8: 168, 15: 5, 2: 89, 19: 193, 14: 78, 12: 306, 7: 34, 4: 9, 9: 823, 6: 276, 3: 368, 0: 903, 18: 4192})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 499/500 [01:19<00:00,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20], Loss: 1.9366\n",
      "Size of clusters: defaultdict(<class 'int'>, {14: 112, 1: 89, 17: 18, 13: 82, 4: 67, 11: 24, 16: 65, 8: 24, 3: 69, 19: 53, 15: 99, 7: 6, 5: 153, 18: 187, 12: 63, 6: 439, 2: 147, 9: 2952, 10: 695, 0: 2656})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 499/500 [01:18<00:00,  6.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20], Loss: 1.8510\n",
      "Size of clusters: defaultdict(<class 'int'>, {9: 96, 1: 80, 19: 48, 11: 80, 3: 80, 18: 96, 6: 128, 12: 160, 16: 400, 0: 800, 10: 496, 4: 384, 15: 304, 5: 576, 14: 449, 8: 799, 2: 929, 13: 560, 7: 960, 17: 575})\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "FEAT_DIM = 512\n",
    "\n",
    "# declare the network\n",
    "model = Backbone(in_chan=3, out_dim=FEAT_DIM, num_classes=NUM_CLASSES)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "# print(model)\n",
    "\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "CLUSTER_EVERY = 500\n",
    "BATCH_SIZE = 16\n",
    "flist = random.sample(flist_full, CLUSTER_EVERY*BATCH_SIZE)\n",
    "\n",
    "num_samples = len(flist)\n",
    "\n",
    "# create random labels\n",
    "labels = np.random.randint(low=0, high=20, size=num_samples)\n",
    "\n",
    "# create the dataset and dataloader\n",
    "NUM_WORKERS = 4\n",
    "train_dataset = PetDataset(flist, transform, labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "running_loss = 0.0\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # get embeddings\n",
    "    embeds = None\n",
    "    for idx, (x, y) in enumerate(tqdm(train_loader)):\n",
    "        \n",
    "        images_ = x.to(device)\n",
    "        labels_ = y.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs, batch_embeds = model(images_)\n",
    "        \n",
    "        # Accumulate the embeddings\n",
    "        batch_embeds = batch_embeds.clone().detach().cpu().numpy()\n",
    "        if embeds is None:\n",
    "            embeds = batch_embeds.copy()\n",
    "        else:\n",
    "            embeds = np.concatenate([embeds, batch_embeds], axis=0)\n",
    "            \n",
    "        loss = criterion(outputs, labels_)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if (idx+1) % CLUSTER_EVERY == 0:\n",
    "            break\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / CLUSTER_EVERY:.4f}')\n",
    "    running_loss = 0.0\n",
    "        \n",
    "    # perform the clustering\n",
    "    labels = cluster_features(embeds, num_clusters=NUM_CLUSTERS)  \n",
    "    \n",
    "    cluster_log = get_sizes(labels)\n",
    "    \n",
    "    print(f\"Size of clusters: {cluster_log}\")\n",
    "    \n",
    "    # re-define the dataset\n",
    "    train_dataset = PetDataset(flist, transform, labels)\n",
    "    train_loader = DataLoader(train_dataset, batch_size = 16, shuffle=True, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1493065d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"pet_data_dc_e20.pth\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573b2034",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
