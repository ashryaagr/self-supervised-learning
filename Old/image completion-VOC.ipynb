{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bdff22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/VOCtrainval_11-May-2012.tar\n",
      "Extracting ./data/VOCtrainval_11-May-2012.tar to ./data\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.transforms import GaussianBlur, ToTensor, ToPILImage, RandomCrop, Compose, Resize, Normalize\n",
    "from torchvision.datasets import CIFAR10, Flickr8k, CelebA\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set hyperparameters\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "image_size = 28\n",
    "\n",
    "# Gaussian Pyramid Constants\n",
    "num_scales = 3\n",
    "scale_factor = 2\n",
    "\n",
    "# Define the transformations to apply to the CIFAR-10 data\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "target_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "])\n",
    "\n",
    "download = True\n",
    "# Define the training and test datasets\n",
    "train_dataset = datasets.VOCDetection(root='./data', image_set=\"train\", download=download, transform=data_transforms, target_transform=target_transform)\n",
    "\n",
    "# Define the dataloaders to load the data in batches during training and testing\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "dataloader = train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9b44ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network architecture with feature pyramid\n",
    "class CompletionModel(nn.Module):\n",
    "    def __init__(self, num_scales=3):\n",
    "        super(CompletionModel, self).__init__()\n",
    "        self.num_scales = num_scales\n",
    "\n",
    "        # Define layers for each scale in the feature pyramid\n",
    "        self.encoders = nn.ModuleList()\n",
    "        self.decoders = nn.ModuleList()\n",
    "        self.channel_reducers = nn.ModuleList()  # 1x1 convolution layers to reduce channels\n",
    "#         self.downsamplers = nn.ModuleList()\n",
    "#         self.upsamplers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_scales):\n",
    "            encoder = nn.Sequential(\n",
    "                nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            self.encoders.append(encoder)\n",
    "\n",
    "            decoder = nn.Sequential(\n",
    "                nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "            self.decoders.append(decoder)\n",
    "            \n",
    "            channel_reducer = nn.Conv2d(3, 256, kernel_size=1)  # 1x1 convolution layer to reduce channels\n",
    "            self.channel_reducers.append(channel_reducer)\n",
    "            \n",
    "            \n",
    "        self.downsampler = nn.Upsample(scale_factor=1/scale_factor, mode='bilinear', align_corners=False)\n",
    "        #self.downsamplers.append(downsampler)\n",
    "            \n",
    "        self.upsampler = nn.Upsample(scale_factor=scale_factor, mode='bilinear', align_corners=False)\n",
    "        #self.upsamplers.append(upsampler)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize list to store features from each scale\n",
    "        features = []\n",
    "\n",
    "        # Forward pass through each scale in the feature pyramid\n",
    "        for i in range(self.num_scales):\n",
    "            encoder_output = self.encoders[i](x)\n",
    "            features.append(encoder_output)\n",
    "            #downsample = nn.Upsample(scale_factor=1/scale_factor**i, mode='bilinear', align_corners=False)\n",
    "            x = self.downsampler(x)\n",
    "            #x = nn.functional.max_pool2d(encoder_output, kernel_size=2, stride=2)\n",
    "\n",
    "#         # Concatenate features from all scales\n",
    "#         concatenated_features = torch.cat(features, dim=1)\n",
    "\n",
    "        # Decode the concatenated features\n",
    "        shape_feats = list(features[-1].shape)\n",
    "        shape_feats[1] = 3\n",
    "        prev_output = torch.zeros(shape_feats).to(device)\n",
    "        \n",
    "#         for f in features:\n",
    "#             print(f.shape)\n",
    "        \n",
    "        for i in range(self.num_scales):\n",
    "            prev_output = self.channel_reducers[i](prev_output)\n",
    "            #print(i, prev_output.shape, features[self.num_scales - i - 1].shape)\n",
    "            prev_output += features[self.num_scales - i - 1]#features[self.num_scales - i - 1]\n",
    "            decoder_output = self.decoders[i](prev_output)\n",
    "            #features.append(encoder_output)\n",
    "            #upsampler = nn.Upsample(scale_factor=scale_factor**i, mode='bilinear', align_corners=False)\n",
    "            if i+1< self.num_scales:\n",
    "                prev_output = self.upsampler(decoder_output)\n",
    "            #x = nn.functional.max_pool2d(encoder_output, kernel_size=2, stride=2)\n",
    "\n",
    "#         decoded = self.decoders[0](concatenated_features)\n",
    "        #print(decoder_output.shape)\n",
    "        return decoder_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49707258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CompletionModel().to(device)\n",
    "\n",
    "# # Define the loss function\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# # Define the optimizer\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee373d83",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/mnt/sphere/home/asagrawal/advancedcv/.env/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/mnt/sphere/home/asagrawal/advancedcv/.env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/mnt/sphere/home/asagrawal/advancedcv/.env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/mnt/sphere/home/asagrawal/advancedcv/.env/lib/python3.8/site-packages/torchvision/datasets/voc.py\", line 204, in __getitem__\n    img, target = self.transforms(img, target)\n  File \"/mnt/sphere/home/asagrawal/advancedcv/.env/lib/python3.8/site-packages/torchvision/datasets/vision.py\", line 96, in __call__\n    target = self.target_transform(target)\n  File \"/mnt/sphere/home/asagrawal/advancedcv/.env/lib/python3.8/site-packages/torchvision/transforms/transforms.py\", line 95, in __call__\n    img = t(img)\n  File \"/mnt/sphere/home/asagrawal/advancedcv/.env/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/mnt/sphere/home/asagrawal/advancedcv/.env/lib/python3.8/site-packages/torchvision/transforms/transforms.py\", line 361, in forward\n    return F.resize(img, self.size, self.interpolation, self.max_size, self.antialias)\n  File \"/mnt/sphere/home/asagrawal/advancedcv/.env/lib/python3.8/site-packages/torchvision/transforms/functional.py\", line 476, in resize\n    _, image_height, image_width = get_dimensions(img)\n  File \"/mnt/sphere/home/asagrawal/advancedcv/.env/lib/python3.8/site-packages/torchvision/transforms/functional.py\", line 78, in get_dimensions\n    return F_pil.get_dimensions(img)\n  File \"/mnt/sphere/home/asagrawal/advancedcv/.env/lib/python3.8/site-packages/torchvision/transforms/_functional_pil.py\", line 31, in get_dimensions\n    raise TypeError(f\"Unexpected type {type(img)}\")\nTypeError: Unexpected type <class 'dict'>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     12\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, _ \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     15\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     17\u001b[0m     occluded_image \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mclone()\n",
      "File \u001b[0;32m/mnt/sphere/home/asagrawal/advancedcv/.env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/mnt/sphere/home/asagrawal/advancedcv/.env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/sphere/home/asagrawal/advancedcv/.env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/mnt/sphere/home/asagrawal/advancedcv/.env/lib/python3.8/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/mnt/sphere/home/asagrawal/advancedcv/.env/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/mnt/sphere/home/asagrawal/advancedcv/.env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/mnt/sphere/home/asagrawal/advancedcv/.env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/mnt/sphere/home/asagrawal/advancedcv/.env/lib/python3.8/site-packages/torchvision/datasets/voc.py\", line 204, in __getitem__\n    img, target = self.transforms(img, target)\n  File \"/mnt/sphere/home/asagrawal/advancedcv/.env/lib/python3.8/site-packages/torchvision/datasets/vision.py\", line 96, in __call__\n    target = self.target_transform(target)\n  File \"/mnt/sphere/home/asagrawal/advancedcv/.env/lib/python3.8/site-packages/torchvision/transforms/transforms.py\", line 95, in __call__\n    img = t(img)\n  File \"/mnt/sphere/home/asagrawal/advancedcv/.env/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/mnt/sphere/home/asagrawal/advancedcv/.env/lib/python3.8/site-packages/torchvision/transforms/transforms.py\", line 361, in forward\n    return F.resize(img, self.size, self.interpolation, self.max_size, self.antialias)\n  File \"/mnt/sphere/home/asagrawal/advancedcv/.env/lib/python3.8/site-packages/torchvision/transforms/functional.py\", line 476, in resize\n    _, image_height, image_width = get_dimensions(img)\n  File \"/mnt/sphere/home/asagrawal/advancedcv/.env/lib/python3.8/site-packages/torchvision/transforms/functional.py\", line 78, in get_dimensions\n    return F_pil.get_dimensions(img)\n  File \"/mnt/sphere/home/asagrawal/advancedcv/.env/lib/python3.8/site-packages/torchvision/transforms/_functional_pil.py\", line 31, in get_dimensions\n    raise TypeError(f\"Unexpected type {type(img)}\")\nTypeError: Unexpected type <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "model = CompletionModel().to(device)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, _ in dataloader:\n",
    "        images = images.to(device)\n",
    "        \n",
    "        occluded_image = images.clone()\n",
    "        _, _, h, w = occluded_image.shape\n",
    "        x = torch.randint(0, w // 2, (1,))\n",
    "        y = torch.randint(0, h // 2, (1,))\n",
    "        occluded_image[:, :, y:y + h // 2, x:x + w // 2] = 0\n",
    "        # Forward pass and loss calculation\n",
    "        completion_images = model(occluded_image.to(device))\n",
    "        loss = criterion(completion_images, images)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Compute the average loss for the epoch\n",
    "    average_loss = running_loss / len(dataloader)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}\")\n",
    "\n",
    "# Generate completions for test images\n",
    "model.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83bae37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# # Define test transformations\n",
    "# test_transform = Compose([\n",
    "#     ToTensor()\n",
    "# ])\n",
    "test_dataset = datasets.VOCDetection(root='./data', split=\"test\", download=download, transform=data_transforms)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=2)\n",
    "\n",
    "# Load CIFAR-10 test dataset\n",
    "# test_dataset = CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "# # Create test data loader\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    i = 0\n",
    "    for images, _ in test_dataloader:\n",
    "        images = images.to(device)\n",
    "        \n",
    "        occluded_image = images.clone()\n",
    "        _, _, h, w = occluded_image.shape\n",
    "        x = torch.randint(0, w // 2, (1,))\n",
    "        y = torch.randint(0, h // 2, (1,))\n",
    "        occluded_image[:, :, y:y + h // 2, x:x + w // 2] = 0\n",
    "        \n",
    "        ########\n",
    "        \n",
    "        # Generate completions using the model\n",
    "        completions = model(occluded_image.to(device))\n",
    "\n",
    "        # Convert tensors to PIL images for visualization\n",
    "        to_pil = ToPILImage()\n",
    "        original_img = to_pil(occluded_image.cpu().squeeze(0))\n",
    "        completed_img = to_pil(completions.cpu().squeeze(0))\n",
    "        \n",
    "        # Define the HTML style for larger images\n",
    "        html_style = \"<style>img { max-width: 100%; height: auto; }</style>\"\n",
    "        i+=1\n",
    "        if i==3:\n",
    "            break\n",
    "\n",
    "        # Display the original and completed images with larger size\n",
    "#         display(HTML(html_style))\n",
    "\n",
    "#         # Display the original and completed images\n",
    "#         display(original_img)\n",
    "#         display(completed_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a7c603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "original_img.resize((original_img.width * 2,original_img.height * 2),\n",
    "                                                    Image.NEAREST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75495678",
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_img.resize((completed_img.width * 2,completed_img.height * 2),\n",
    "                                                    Image.NEAREST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dca5b3",
   "metadata": {},
   "source": [
    "## Downstream evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29299bd3",
   "metadata": {},
   "source": [
    "In the SSL model trained above, we have multiple encoders. All of these specialize in capturing the information at a specific level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9238f9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'ssl_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "553a45e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('ssl_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "df3462c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (1): Flatten(start_dim=1, end_dim=-1)\n",
       "  (2): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nn.Sequential(model.encoders[0], nn.Flatten(), nn.C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0b62fed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 128, 128])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.encoders[2](torch.randn(1, 3, 128, 128).cuda()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2043fe73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (4): ReLU()\n",
      "  (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (7): ReLU()\n",
      "  (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (9): Flatten(start_dim=1, end_dim=-1)\n",
      "  (10): Dropout(p=0.5, inplace=False)\n",
      "  (11): Linear(in_features=65536, out_features=1024, bias=True)\n",
      "  (12): ReLU()\n",
      "  (13): Dropout(p=0.5, inplace=False)\n",
      "  (14): Linear(in_features=1024, out_features=102, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_shape = (256, 128, 128)\n",
    "# Define the classification model\n",
    "classification_model = nn.Sequential(\n",
    "    # Convolutional layers\n",
    "    nn.Conv2d(input_shape[0], 64, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm2d(256),\n",
    "\n",
    "#     # Flatten the feature maps\n",
    "    nn.Flatten(),\n",
    "    nn.Dropout(),\n",
    "#     # Fully connected layers\n",
    "    nn.Linear(256 * 16*16, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(1024, 102)  # num_classes is the number of classes for classification\n",
    ")\n",
    "\n",
    "# Print the classification model architecture\n",
    "print(classification_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92e7d860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification_model.cuda()(model.encoders[2](torch.randn(1, 3, 128, 128).cuda())).shape\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9da7e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(classification_model.parameters(), lr=learning_rate, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c17985ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(model, classification_model, dataloader):\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            #outputs = model(images)\n",
    "            outputs = classification_model.cuda()(model.encoders[2](images))\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e156816c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Loss: 99.91639947891235 Accuracy: 13.431372549019608\n",
      "Epoch 1 : Test Accuracy: 1.6588063099691006\n",
      "Epoch 2 : Loss: 73.51753616333008 Accuracy: 33.333333333333336\n",
      "Epoch 2 : Test Accuracy: 2.5532606927955763\n",
      "Epoch 3 : Loss: 46.0807865858078 Accuracy: 64.50980392156863\n",
      "Epoch 3 : Test Accuracy: 3.512766303463978\n",
      "Epoch 4 : Loss: 21.646090149879456 Accuracy: 83.92156862745098\n",
      "Epoch 4 : Test Accuracy: 3.8542852496340867\n",
      "Epoch 5 : Loss: 10.291690200567245 Accuracy: 92.84313725490196\n",
      "Epoch 5 : Test Accuracy: 3.7567084078711988\n",
      "Epoch 6 : Loss: 5.119639813899994 Accuracy: 93.82352941176471\n",
      "Epoch 6 : Test Accuracy: 3.870548056594568\n",
      "Epoch 7 : Loss: 3.0966795682907104 Accuracy: 96.17647058823529\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "classification_model = classification_model.to(device)\n",
    "\n",
    "log_train_every = 1\n",
    "log_test_every = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #outputs = model(inputs)\n",
    "        outputs = classification_model.cuda()(model.encoders[2](inputs))\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    # compute training & testing accuracy every couple of iterations        \n",
    "    if (epoch+1) % log_train_every == 0:\n",
    "        train_accuracy = model_eval(model, classification_model, train_loader)\n",
    "\n",
    "        # Log the loss\n",
    "        #writer.add_scalar('Loss/train', loss.cpu().item(), epoch * len(train_loader) + i)\n",
    "\n",
    "        # Log the training accuracy\n",
    "        #writer.add_scalar('Accuracy/train', train_accuracy, epoch * len(train_loader) + i)\n",
    "        print(f\"Epoch {epoch+1} : Loss: {running_loss} Accuracy: {train_accuracy}\")\n",
    "        \n",
    "    if (epoch+1) % log_test_every == 0:\n",
    "        test_accuracy = model_eval(model, classification_model, test_dataloader)\n",
    "\n",
    "        # Log the test accuracy\n",
    "        #writer.add_scalar('Accuracy/test', test_accuracy, epoch * len(train_loader) + i)\n",
    "        print(f\"Epoch {epoch+1} : Test Accuracy: {test_accuracy}\")\n",
    "        \n",
    "# writer.close()\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c5513e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My acv",
   "language": "python",
   "name": "acv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
