{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bdff22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import GaussianBlur, ToTensor, ToPILImage, RandomCrop, Compose, Resize, Normalize\n",
    "from torchvision.datasets import CIFAR10, Flickr8k, CelebA\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set hyperparameters\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "image_size = 28\n",
    "\n",
    "# Gaussian Pyramid Constants\n",
    "num_scales = 3\n",
    "scale_factor = 2\n",
    "\n",
    "# Define data transformations\n",
    "transform = Compose([\n",
    "    RandomCrop(image_size),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Create data loader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9b44ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network architecture with feature pyramid\n",
    "class CompletionModel(nn.Module):\n",
    "    def __init__(self, num_scales=3):\n",
    "        super(CompletionModel, self).__init__()\n",
    "        self.num_scales = num_scales\n",
    "\n",
    "        # Define layers for each scale in the feature pyramid\n",
    "        self.encoders = nn.ModuleList()\n",
    "        self.decoders = nn.ModuleList()\n",
    "        self.channel_reducers = nn.ModuleList()  # 1x1 convolution layers to reduce channels\n",
    "#         self.downsamplers = nn.ModuleList()\n",
    "#         self.upsamplers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_scales):\n",
    "            encoder = nn.Sequential(\n",
    "                nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            self.encoders.append(encoder)\n",
    "\n",
    "            decoder = nn.Sequential(\n",
    "                nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "            self.decoders.append(decoder)\n",
    "            \n",
    "            channel_reducer = nn.Conv2d(3, 256, kernel_size=1)  # 1x1 convolution layer to reduce channels\n",
    "            self.channel_reducers.append(channel_reducer)\n",
    "            \n",
    "            \n",
    "        self.downsampler = nn.Upsample(scale_factor=1/scale_factor, mode='bilinear', align_corners=False)\n",
    "        #self.downsamplers.append(downsampler)\n",
    "            \n",
    "        self.upsampler = nn.Upsample(scale_factor=scale_factor, mode='bilinear', align_corners=False)\n",
    "        #self.upsamplers.append(upsampler)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize list to store features from each scale\n",
    "        features = []\n",
    "\n",
    "        # Forward pass through each scale in the feature pyramid\n",
    "        for i in range(self.num_scales):\n",
    "            encoder_output = self.encoders[i](x)\n",
    "            features.append(encoder_output)\n",
    "            #downsample = nn.Upsample(scale_factor=1/scale_factor**i, mode='bilinear', align_corners=False)\n",
    "            x = self.downsampler(x)\n",
    "            #x = nn.functional.max_pool2d(encoder_output, kernel_size=2, stride=2)\n",
    "\n",
    "#         # Concatenate features from all scales\n",
    "#         concatenated_features = torch.cat(features, dim=1)\n",
    "\n",
    "        # Decode the concatenated features\n",
    "        shape_feats = list(features[-1].shape)\n",
    "        shape_feats[1] = 3\n",
    "        prev_output = torch.zeros(shape_feats).to(device)\n",
    "        \n",
    "#         for f in features:\n",
    "#             print(f.shape)\n",
    "        \n",
    "        for i in range(self.num_scales):\n",
    "            prev_output = self.channel_reducers[i](prev_output)\n",
    "            #print(i, prev_output.shape, features[self.num_scales - i - 1].shape)\n",
    "            prev_output += features[self.num_scales - i - 1]#features[self.num_scales - i - 1]\n",
    "            decoder_output = self.decoders[i](prev_output)\n",
    "            #features.append(encoder_output)\n",
    "            #upsampler = nn.Upsample(scale_factor=scale_factor**i, mode='bilinear', align_corners=False)\n",
    "            if i+1< self.num_scales:\n",
    "                prev_output = self.upsampler(decoder_output)\n",
    "            #x = nn.functional.max_pool2d(encoder_output, kernel_size=2, stride=2)\n",
    "\n",
    "#         decoded = self.decoders[0](concatenated_features)\n",
    "        #print(decoder_output.shape)\n",
    "        return decoder_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee373d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0108\n",
      "Epoch [2/10], Loss: 0.0068\n",
      "Epoch [3/10], Loss: 0.0065\n",
      "Epoch [4/10], Loss: 0.0063\n",
      "Epoch [5/10], Loss: 0.0061\n",
      "Epoch [6/10], Loss: 0.0060\n",
      "Epoch [7/10], Loss: 0.0059\n",
      "Epoch [8/10], Loss: 0.0059\n",
      "Epoch [9/10], Loss: 0.0058\n",
      "Epoch [10/10], Loss: 0.0058\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletionModel(\n",
       "  (encoders): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (decoders): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): Tanh()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): Tanh()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (channel_reducers): ModuleList(\n",
       "    (0): Conv2d(3, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): Conv2d(3, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (2): Conv2d(3, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (downsampler): Upsample(scale_factor=0.5, mode=bilinear)\n",
       "  (upsampler): Upsample(scale_factor=2.0, mode=bilinear)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CompletionModel().to(device)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, _ in dataloader:\n",
    "        images = images.to(device)\n",
    "        \n",
    "        occluded_image = images.clone()\n",
    "        _, _, h, w = occluded_image.shape\n",
    "        x = torch.randint(0, w // 2, (1,))\n",
    "        y = torch.randint(0, h // 2, (1,))\n",
    "        occluded_image[:, :, y:y + h // 2, x:x + w // 2] = 0\n",
    "        # Forward pass and loss calculation\n",
    "        completion_images = model(occluded_image.to(device))\n",
    "        loss = criterion(completion_images, images)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Compute the average loss for the epoch\n",
    "    average_loss = running_loss / len(dataloader)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}\")\n",
    "\n",
    "# Generate completions for test images\n",
    "model.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bae37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# Define test transformations\n",
    "test_transform = Compose([\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 test dataset\n",
    "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "# Create test data loader\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, _ in test_dataloader:\n",
    "        images = images.to(device)\n",
    "        \n",
    "        occluded_image = images.clone()\n",
    "        _, _, h, w = occluded_image.shape\n",
    "        x = torch.randint(0, w // 2, (1,))\n",
    "        y = torch.randint(0, h // 2, (1,))\n",
    "        occluded_image[:, :, y:y + h // 2, x:x + w // 2] = 0\n",
    "        \n",
    "        ########\n",
    "        \n",
    "        # Generate completions using the model\n",
    "        completions = model(occluded_image.to(device))\n",
    "\n",
    "        # Convert tensors to PIL images for visualization\n",
    "        to_pil = ToPILImage()\n",
    "        original_img = to_pil(occluded_image.cpu().squeeze(0))\n",
    "        completed_img = to_pil(completions.cpu().squeeze(0))\n",
    "        \n",
    "        # Define the HTML style for larger images\n",
    "        html_style = \"<style>img { max-width: 100%; height: auto; }</style>\"\n",
    "\n",
    "        # Display the original and completed images with larger size\n",
    "#         display(HTML(html_style))\n",
    "\n",
    "#         # Display the original and completed images\n",
    "#         display(original_img)\n",
    "#         display(completed_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a7c603c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAAFACAIAAABC8jL9AAAQtklEQVR4nO3dWW9b+XnHcR7ykBQlarWssbzKlhd5xvE2HqceZzxBOlmaFGiCTJcAvSrQmxToy+gLKOZFtE2ABkmDaVo0KIpZOkuCepEtb2M7XiXKsmRLpLid01fQry/TR/P93P7Aw0OKP52bB88/2T1zsPB/K5aKkE5Obod0qD4K6fjYFKTPnj2HtLH8ENKjc7OQnj51CtJDs/RtZL0upL1OD9JOexPSf/v3X0J688YNSJO8Bmm3Tffc6a5BOjRUhnTH1DRduQ1hoddNIF3fbELa6tA3Wa7QPSeFDNKsS99VIc8h7PXptZVqFdKh+gikrVYHUuqnpP/nLLAUmAWWArPAUmAWWArMAkuBWWApMAssBWaBpcDSQp/mh9KUpnxe2U6zOMPD45BWqoOQTu/YBenxk38C6VfPnoC0kNE8zcaLF5CmxRKk7WYL0iuXL0O6+OQJpC+er0NaS+kTFbo0EjU1XIH02LFDkG5u0uzRrVuPIW226J6HhuqQ1tIBSFvrNMVVwEmsckLzYeVSCunw6DZI68M0ldinmyqUR+nz+gSWArPAUmAWWArMAkuBWWApMAssBWaBpcAssBSYBZYCS7MiTZ90+31I7927B+n4CO1bev007aY6dox2U01tox1C0xM09bLZpsmkJ/fvQtrp0nai6WmaSztx5jik8zeuQvrocQPSUoGmmtKc7nnvJG0m+/43L0A6VB+D9DcXv4D0vz68BGkTp6nG6zSnVaKBqMJmi6680dyAtDpAs4PbJndAOjJOt7WyShvgasP0a/cJLAVmgaXALLAUmAWWArPAUmAWWArMAkuBWWApMAssBZZ2X1JhWtczNUlbr95+8xykJ79yFNID+2gn1oP79yG9+MmnkI5NTEDa3KDtU1M7adpmz/59kOY08Fb4wZ/+ENL1Ndq2dW9+gS7dp9emGU3adfHb2LWfToHcPE6bnNbTYUiXGyuQPn2yCGl7g2atSlU6u3DX3t2QJintD0sH6BPVcOJtZJx+kwO4IcwnsBSYBZYCs8BSYBZYCswCS4FZYCkwCywFZoGlwCywFFgyvX8G4hqeyPbOBdqZ9Oc/+D5dOaWT/qplStebNCF06Rrtl9pzgLZt7T88B+nUNE1iJfjPsN+j3VSt9U1IL35GG6R+/tN/hrS7TuctdjZWIR2k0aPC0cMHIH2e0V/waZ/mtMamaA6vhru4uj3aEMYb0YZ52xZ2ISvQ5y3ia5Mi/XTynKYhfQJLgVlgKTALLAVmgaXALLAUmAWWArPAUmAWWArMAkuBpWlOkyt5vwvpygqdmre6SpuNJvbsoSs3liB9tPQE0na3B2m1VqO72kZbvkolWmyV4CmBRZw86xXptT/80Z9B+nvz6w9+33ewRfzd3/8jpBnuLfMJLAVmgaXALLAUmAWWArPAUmAWWArMAkuBWWApMAssBZZW0yrEgwO0Funh/YeQ/tNPfgrp1DY8JRA3ObW6NJuy7/AhSBtLND22+Ig+0YFZ2gJVSum/Ya9Pm42qNVw/pS0NfziFvEDzfz6BpcAssBSYBZYCs8BSYBZYCswCS4FZYCkwCywFZoGlwNKpiZ0Q1wZoTivJafvUsxU6Q3BleQ3SrE9X3jEzA+npN85BOlqnc/EWHz+CtNdpQrpr725I66NjkFYqtKlLW1tapFmrHI+99AksBWaBpcAssBSYBZYCs8BSYBZYCswCS4FZYCkwCywFlo4NT1KM63p4gqSEJ+4VctoRleW09erI0WOUzh2lu0poxmtthU5FfPjwAaQPcJ/W7r0zkE5P74VUW1taopYl2DKfwFJgFlgKzAJLgVlgKTALLAVmgaXALLAUmAWWArPAUmBpWkohrlbKkPKMSKlEaZbRJFatSu+7f4bmlkZGaOtVvT4E6c5d05AefpUmwFZXVyldox1gT1dWINXWtrr2DNI8p4lGn8BSYBZYCswCS4FZYCkwCywFZoGlwCywFJgFlgKzwFJgaalM81Jplea0SrjLh/dp9fu09WpgkKapJraNQzo4SCf9JQl93qRYgnRogK48OjYG6W78vO12G1Jtbe//688g5ZlFn8BSYBZYCswCS4FZYCkwCywFZoGlwCywFJgFlgKzwFJgaWNlEeIizloVcSdWoUC7fHI8nfDUqROQjk2M4vsSngDjqRf+RL0+nXvIE2DlMk2AaWtrLNG5lvjD8QksRWaBpcAssBSYBZYCs8BSYBZYCswCS4FZYCkwCywFlj54+DvKcQqE54fKFdqndeL4cUi//o0LkI6Pj0HaarUg7fEurmoV0jSlT/SSU+R4F1ePpri0xWX0y8lw/s8nsBSYBZYCs8BSYBZYCswCS4FZYCkwCywFZoGlwCywFFjKFeatV0mRXjx3ZA7Sd999F9JDBw/R++JsytqzVUjLlTKkw0NDkJZKNHlWwjkt3myEC8K0xXXam5jTT8cnsBSYBZYCs8BSYBZYCswCS4FZYCkwCywFZoGlwCywFFja6XUhLmHDawODkB579SuQzu6nWatiRjNPWbcNabu5AWl9chLSarkCKe/ESvAkxz7uPeoXPZ3wyyvPOpDyvKNPYCkwCywFZoGlwCywFJgFlgKzwFJgFlgKzAJLgVlgKbD0G299DeJyShuklpeWIK3iZFLrxRqkJdxNNVDFiaiBGqT9Lk2e9bt0SiBvveLFVhmeXZhnLsX68iri1qvEnVjSVmWBpcAssBSYBZYCs8BSYBZYCswCS4FZYCkwCywFlv74r/8K4jLOHq0sP4W0hHueqglNJnFaKtDcUrlWhbSA5y32Oi18Lb1vEb+rHE8nTHBjlra2FLde5U5iSVuVBZYCs8BSYBZYCswCS4FZYCkwCywFZoGlwCywFFia5n2Iv7h+E9LOJp0SeHB2FtJ6heaWihntplp8+BjS0clxSCe3b4c0wQmwfpdOkcvwm3zJGXOuxPoSq+EWtw5ucfMJLAVmgaXALLAUmAWWArPAUmAWWArMAkuBWWApMAssBZY2HtFU0+effArp5YWrkL7zrXcg/YMzZyG99PlvIf3gk88gPXP+TUjPvnEG0snJbZDWR+qQpnhmYrFMG8J6NMSlLW72wAFIry1ch9QnsBSYBZYCs8BSYBZYCswCS4FZYCkwCywFZoGlwCywFFhaLpUhHqsPQ7rSaED6pLEI6QaeAzh/bR7Szy5ehPTB2jpdef4GpLMz+yB97bVXIT1wkOZpavVBSAuJ/0m/vGb374f01s3bkPq7kQKzwFJgFlgKzAJLgVlgKTALLAVmgaXALLAUmAWWAksbq6sQz80cgvTF6XOQnjgwB+lwShNgrc0NStsvIF24cRnSO7dpEmv7+CSkt27egfTM67Tla+fO3ZDu3rcHUm1tF75GPbqxsACpT2ApMAssBWaBpcAssBSYBZYCs8BSYBZYCswCS4FZYCmw5DsX3ob43W9+G9JyQpdudpuQzh6iPUB3btMeoH/4xc8hvf67+5DmOd10ktB82MjQGKTTU7sg3TG1A9Jde+i1r+x8BdJKpQLp3bt3Ib1y5RKkS40n+L50GuO3v0W/nD/6znchXV5ehvTOPfptvPX2W5DOHaXpwGKJfht5RkdIbjbp1770hPbDtVsdSD/84GNIfQJLgVlgKTALLAVmgaXALLAUmAWWArPAUmAWWArMAkuBJQf30Hl8B/fQrqY0zSFdb65C+ofnz0N6GDdI/edHNJvyq48+hLSdZZD2EprF6dIoTiHB/4ZFTCspTTVVB6qUVoYgHahSyvfcbNE5j2mZXnvy+AlId2yfgpQnwNaer0L6N3/7Y0jPnD0NaV7A30aX5qWer6xAevl/aOJtYmwC0izj35WksCywFJgFlgKzwFJgFlgKzAJLgVlgKTALLAVmgaXA0laP5ksu3rpOry7SJFYpobmWxi9+CekAbq7q9HqQdnO6q35GKeP/dlmBBrX6OOWz2adP1HrRgjTvr0F68MBhSE8df53uqkm/jeWlp5Deuf0Q0ivzVyDt97uQ1odqkDYWcfvUBn2TPFuWd+mu1lfor1Do0V+/Vhmg1xZpT5tPYCkwCywFZoGlwCywFJgFlgKzwFJgFlgKzAJLgVlgKbA0xS1QeZEbjlNNfUrX1jYgXcXNVYUShSxN6BMlOAGG91TICvRamrR62cl3RfyekxKly0t0VmOnTXNaXz37JqTNDZpMetJ4BOmla59DurAwT+/bpP1h1xeuQXrq5HFIx0dHIH26tATp0v3HdOWhUUjrA3VI2z3+bUgKywJLgVlgKTALLAVmgaXALLAUmAWWArPAUmAWWAosHS3RXMtL9kfh9qlCiSaTCjgRlec09ZTzvx1826yPV8aNWTyJ1cUvYxP3eGU5Tduk+E2WSvR1tJu0q+njj/8D0mKBZq2OvXYS0uPHZiE9cngHpD/L6X2vXaVZq6tXaNvW4uMLkNYqFUjvfnEH0qcP6ETFuSNzkOIoXSHv0i/HJ7AUmAWWArPAUmAWWArMAkuBWWApMAssBWaBpcAssBRYWsc5nqREJ6PluLmK9zzlvOcpobVXRZ7xwivneGWePMtwQ1izS2f5JXjpbob/SXFvWYJTawMV+rxJbxPSi59/SFfG0yePfO+PIT185Bik9SpNB7733nuQ3rt9C9L5i5cgHR0ahnR1ZRXS5y9eQNrD0yc3Nmg/3OPGMqQ+gaXALLAUmAWWArPAUmAWWArMAkuBWWApMAssBWaBpcDSFEeESkU+uxDP8qNBrAIunyrw/qliD6e4cFNXKaXZMr4r/ECFHs5p9VOaLurjG2/26Nt4yVjaS6blaPvU0DDd8/gAbZDKmuuQPrpL+6XWlukcwDJ+3nanDenGOs1LlVKaWpuZPQBpfXAQ0laPpvQaT55D+ulvfgupT2ApMAssBWaBpcAssBSYBZYCs8BSYBZYCswCS4FZYCmwtIuTWD3ct1TgFKepMkz7uEOogyNROKZVyLq0Beol42G4P6yL30YbTyds4Sdq45xWKaMrj9UGIN25cxrSs6dOQHrwIJ0/OD9PpwQuLNyE9PbtG5BubNB5i/XREUiHB2m2LCnSn2GgVoU079FrF65chfT67duQXrpGpzH6BJYCs8BSYBZYCswCS4FZYCkwCywFZoGlwCywFJgFlgJL9k/TLA6fMMjbp3gpVtbHK+Peoz5OTHVxC1SG95zj+2Z45f5L5tJIsUjbmCZGxiCdntwO6cye3ZDyPq3lFToX79naKqQPGw1I1zdpc1WKm7pGyvRdnTt1GtIf/cVfQtrq0F/wo//+FNK7d+hUxGerzyBdbzYh7eCMl09gKTALLAVmgaXALLAUmAWWArPAUmAWWArMAkuBWWApsJS3QPFsEc9L8dmFfR7iwqmmfh9T3CDFs1Y5zmn18H0LPMWFn2ikTGcmzu7cAWm1QqcE3r5B26cWV1YgfbZOJwxu8pQePhqGB+meZ3bRdOD50ychfevsOUi/uEXfxvu/+jWkdx48grST0/mD/Nso44mZxSLt8fIJLAVmgaXALLAUmAWWArPAUmAWWArMAkuBWWApMAssBZa8Nj2FOYZ4smEf57h6eP5gH7dPdXv0f4fPLuTNVTyXxneVveTVlFYwLfM5jz38RDn+jy7RlE+vSK/t4T1X8czEo9O0x+v8G3Qq4uFD+yFdbDyF9Cf/8j699vkGpHlK5zxyVdISzlrh2FqCf0GfwFJgFlgKzAJLgVlgKTALLAVmgaXALLAUmAWWArPAUmD/C6fkOumrYEY8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=320x320 at 0x7F94693305E0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "original_img.resize((original_img.width * 10,original_img.height * 10),\n",
    "                                                    Image.NEAREST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75495678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=320x320 at 0x7F9469330340>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completed_img.resize((completed_img.width * 10,completed_img.height * 10),\n",
    "                                                    Image.NEAREST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02a6117",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c672b7",
   "metadata": {},
   "source": [
    "Can try with other datasets as well available here: https://pytorch.org/vision/0.9/datasets.html\n",
    "\n",
    "Currently, CelebA hit some download limit so couldn't try. can try later again"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
